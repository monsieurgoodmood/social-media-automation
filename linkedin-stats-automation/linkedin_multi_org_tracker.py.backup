#!/usr/bin/env python3
"""
LinkedIn Multi-Organization Statistics Tracker
Version am√©lior√©e avec mise √† jour forc√©e pour automatisation
"""

import os
import json
import requests
import urllib.parse
import random
from pathlib import Path
from datetime import datetime, timedelta
import time
from dateutil import parser
import pytz
import sys
from dotenv import load_dotenv

# Pour Google Sheets
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# Chargement des variables d'environnement
load_dotenv()

def get_column_letter(col_idx):
    """Convertit un indice de colonne (0-based) en lettre de colonne pour Google Sheets"""
    result = ""
    col_idx = col_idx + 1  # Convertir de 0-based √† 1-based
    while col_idx > 0:
        col_idx, remainder = divmod(col_idx - 1, 26)
        result = chr(65 + remainder) + result
    return result

class LinkedInDailyPageStatisticsTracker:
    """Classe pour suivre les statistiques quotidiennes des vues de page LinkedIn"""
    
    def __init__(self, access_token, organization_id, days_history=365, sheet_name=None):
        """Initialise le tracker avec le token d'acc√®s et l'ID de l'organisation"""
        self.access_token = access_token
        self.organization_id = organization_id
        self.days_history = days_history
        self.sheet_name = sheet_name or f"LinkedIn_Daily_Stats_{organization_id}"
        self.base_url = "https://api.linkedin.com/v2"
        
    def get_headers(self):
        """Retourne les en-t√™tes pour les requ√™tes API"""
        return {
            "Authorization": f"Bearer {self.access_token}",
            "X-Restli-Protocol-Version": "2.0.0",
            "LinkedIn-Version": "202312",
            "Content-Type": "application/json"
        }
    
    def get_daily_page_statistics(self):
        """Obtient les statistiques quotidiennes de vues de page pour l'organisation"""
        # Encoder l'URN de l'organisation
        organization_urn = f"urn:li:organization:{self.organization_id}"
        encoded_urn = urllib.parse.quote(organization_urn)
        
        # Calculer les timestamps (millisecondes depuis l'√©poque)
        end_date = datetime.now()
        start_date = end_date - timedelta(days=self.days_history)
        
        # Convertir en millisecondes depuis l'√©poque
        start_timestamp = int(start_date.timestamp() * 1000)
        end_timestamp = int(end_date.timestamp() * 1000)
        
        # Construire l'URL avec le format RESTli 2.0
        url = (f"{self.base_url}/organizationPageStatistics?q=organization&"
               f"organization={encoded_urn}&"
               f"timeIntervals=(timeRange:(start:{start_timestamp},end:{end_timestamp}),"
               f"timeGranularityType:DAY)")
        
        # Effectuer la requ√™te avec gestion des erreurs et retry
        max_retries = 3
        retry_delay = 2  # secondes
        
        for attempt in range(max_retries):
            try:
                response = requests.get(url, headers=self.get_headers())
                
                if response.status_code == 200:
                    data = response.json()
                    print(f"   Donn√©es de statistiques quotidiennes des pages r√©cup√©r√©es avec succ√®s")
                    return data
                    
                elif response.status_code == 429:
                    # Rate limit, attendre avant de r√©essayer
                    print(f"   Rate limit atteint, attente de {retry_delay} secondes...")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # Backoff exponentiel
                else:
                    print(f"   Erreur API: {response.status_code} - {response.text}")
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    
            except Exception as e:
                print(f"   Exception lors de la requ√™te: {e}")
                time.sleep(retry_delay)
                retry_delay *= 2
        
        print("   √âchec apr√®s plusieurs tentatives pour obtenir les statistiques quotidiennes des pages.")
        return None
    
    def parse_daily_page_statistics(self, data):
        """Analyse les donn√©es quotidiennes de l'API et extrait les statistiques pertinentes"""
        daily_stats = []
        
        # S'assurer que les donn√©es sont valides
        if not data or 'elements' not in data or len(data['elements']) == 0:
            print("   Aucune donn√©e de statistiques quotidiennes de pages valide trouv√©e.")
            return daily_stats
        
        # Parcourir chaque √©l√©ment (un par jour)
        for element in data['elements']:
            # Ignorer les √©l√©ments sans plage de temps ou statistiques
            if 'timeRange' not in element or 'totalPageStatistics' not in element:
                continue
            
            time_range = element['timeRange']
            stats = element['totalPageStatistics']
            
            # Convertir le timestamp en date lisible
            # Le timestamp est en millisecondes depuis l'√©poque
            start_date = datetime.fromtimestamp(time_range['start'] / 1000)
            date_str = start_date.strftime('%Y-%m-%d')
            
            # Extraire les vues pour chaque type
            views = stats.get('views', {})
            
            # Extraire les donn√©es de vues pertinentes
            all_page_views = views.get('allPageViews', {}).get('pageViews', 0)
            unique_page_views = views.get('allPageViews', {}).get('uniquePageViews', 0)
            
            all_desktop_views = views.get('allDesktopPageViews', {}).get('pageViews', 0)
            unique_desktop_views = views.get('allDesktopPageViews', {}).get('uniquePageViews', 0)
            
            all_mobile_views = views.get('allMobilePageViews', {}).get('pageViews', 0)
            unique_mobile_views = views.get('allMobilePageViews', {}).get('uniquePageViews', 0)
            
            # D√©tails par type de page
            overview_views = views.get('overviewPageViews', {}).get('pageViews', 0)
            unique_overview_views = views.get('overviewPageViews', {}).get('uniquePageViews', 0)
            
            about_views = views.get('aboutPageViews', {}).get('pageViews', 0)
            unique_about_views = views.get('aboutPageViews', {}).get('uniquePageViews', 0)
            
            people_views = views.get('peoplePageViews', {}).get('pageViews', 0)
            unique_people_views = views.get('peoplePageViews', {}).get('uniquePageViews', 0)
            
            jobs_views = views.get('jobsPageViews', {}).get('pageViews', 0)
            unique_jobs_views = views.get('jobsPageViews', {}).get('uniquePageViews', 0)
            
            careers_views = views.get('careersPageViews', {}).get('pageViews', 0)
            unique_careers_views = views.get('careersPageViews', {}).get('uniquePageViews', 0)
            
            # Nouveaux champs extraits de la documentation
            desktop_careers_views = views.get('desktopCareersPageViews', {}).get('pageViews', 0)
            desktop_jobs_views = views.get('desktopJobsPageViews', {}).get('pageViews', 0)
            desktop_overview_views = views.get('desktopOverviewPageViews', {}).get('pageViews', 0)
            desktop_life_at_views = views.get('desktopLifeAtPageViews', {}).get('pageViews', 0)
            
            mobile_careers_views = views.get('mobileCareersPageViews', {}).get('pageViews', 0)
            mobile_jobs_views = views.get('mobileJobsPageViews', {}).get('pageViews', 0)
            mobile_overview_views = views.get('mobileOverviewPageViews', {}).get('pageViews', 0)
            mobile_life_at_views = views.get('mobileLifeAtPageViews', {}).get('pageViews', 0)
            
            life_at_views = views.get('lifeAtPageViews', {}).get('pageViews', 0)
            unique_life_at_views = views.get('lifeAtPageViews', {}).get('uniquePageViews', 0)
            
            # Extraire les donn√©es des clics sur les boutons personnalis√©s
            clicks = stats.get('clicks', {})
            desktop_custom_button_clicks = clicks.get('desktopCustomButtonClickCounts', [])
            mobile_custom_button_clicks = clicks.get('mobileCustomButtonClickCounts', [])
            
            # Calculer le total des clics sur les boutons personnalis√©s
            desktop_button_clicks_total = sum([click.get('count', 0) for click in desktop_custom_button_clicks]) if desktop_custom_button_clicks else 0
            mobile_button_clicks_total = sum([click.get('count', 0) for click in mobile_custom_button_clicks]) if mobile_custom_button_clicks else 0
            
            # Stocker les donn√©es du jour
            day_stats = {
                'date': date_str,
                # Vue g√©n√©rales
                'total_views': all_page_views,
                'unique_views': unique_page_views,
                # Vues desktop et mobile
                'desktop_views': all_desktop_views,
                'unique_desktop_views': unique_desktop_views,
                'mobile_views': all_mobile_views,
                'unique_mobile_views': unique_mobile_views,
                # Vues par type de page
                'overview_views': overview_views,
                'unique_overview_views': unique_overview_views,
                'about_views': about_views,
                'unique_about_views': unique_about_views,
                'people_views': people_views,
                'unique_people_views': unique_people_views,
                'jobs_views': jobs_views,
                'unique_jobs_views': unique_jobs_views,
                'careers_views': careers_views,
                'unique_careers_views': unique_careers_views,
                # Nouveaux champs d√©taill√©s
                'desktop_careers_views': desktop_careers_views,
                'desktop_jobs_views': desktop_jobs_views,
                'desktop_overview_views': desktop_overview_views,
                'desktop_life_at_views': desktop_life_at_views,
                'mobile_careers_views': mobile_careers_views,
                'mobile_jobs_views': mobile_jobs_views,
                'mobile_overview_views': mobile_overview_views,
                'mobile_life_at_views': mobile_life_at_views,
                'life_at_views': life_at_views,
                'unique_life_at_views': unique_life_at_views,
                # Clics sur les boutons personnalis√©s
                'desktop_button_clicks': desktop_button_clicks_total,
                'mobile_button_clicks': mobile_button_clicks_total,
                'total_button_clicks': desktop_button_clicks_total + mobile_button_clicks_total
            }
            
            daily_stats.append(day_stats)
        
        # Trier les statistiques par date (plus ancien au plus r√©cent)
        daily_stats.sort(key=lambda x: x['date'], reverse=False)
        
        return daily_stats


class LinkedInFollowerStatisticsTracker:
    """Classe pour suivre les statistiques quotidiennes des followers LinkedIn"""
    
    def __init__(self, access_token, organization_id, days_history=365):
        """Initialise le tracker avec le token d'acc√®s et l'ID de l'organisation"""
        self.access_token = access_token
        self.organization_id = organization_id
        self.days_history = min(days_history, 365)  # Maximum 12 mois selon la documentation
        self.base_url = "https://api.linkedin.com/v2"
        
    def get_headers(self):
        """Retourne les en-t√™tes pour les requ√™tes API"""
        return {
            "Authorization": f"Bearer {self.access_token}",
            "X-Restli-Protocol-Version": "2.0.0",
            "LinkedIn-Version": "202312",
            "Content-Type": "application/json"
        }
    
    def get_daily_follower_statistics(self):
        """Obtient les statistiques quotidiennes des followers pour l'organisation"""
        # Encoder l'URN de l'organisation
        organization_urn = f"urn:li:organization:{self.organization_id}"
        encoded_urn = urllib.parse.quote(organization_urn)
        
        # Calculer les timestamps (millisecondes depuis l'√©poque)
        end_date = datetime.now()
        start_date = end_date - timedelta(days=self.days_history)
        
        # Convertir en millisecondes depuis l'√©poque
        start_timestamp = int(start_date.timestamp() * 1000)
        end_timestamp = int(end_date.timestamp() * 1000)
        
        # Construire l'URL avec le format RESTli 2.0
        url = (f"{self.base_url}/organizationalEntityFollowerStatistics?q=organizationalEntity&"
               f"organizationalEntity={encoded_urn}&"
               f"timeIntervals=(timeRange:(start:{start_timestamp},end:{end_timestamp}),"
               f"timeGranularityType:DAY)")
        
        # Effectuer la requ√™te avec gestion des erreurs et retry
        max_retries = 3
        retry_delay = 2  # secondes
        
        for attempt in range(max_retries):
            try:
                response = requests.get(url, headers=self.get_headers())
                
                if response.status_code == 200:
                    data = response.json()
                    print(f"   Donn√©es de statistiques quotidiennes des followers r√©cup√©r√©es avec succ√®s")
                    return data
                    
                elif response.status_code == 429:
                    # Rate limit, attendre avant de r√©essayer
                    print(f"   Rate limit atteint, attente de {retry_delay} secondes...")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # Backoff exponentiel
                else:
                    print(f"   Erreur API: {response.status_code} - {response.text}")
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    
            except Exception as e:
                print(f"   Exception lors de la requ√™te: {e}")
                time.sleep(retry_delay)
                retry_delay *= 2
        
        print("   √âchec apr√®s plusieurs tentatives pour obtenir les statistiques quotidiennes des followers.")
        return None
    
    def parse_daily_follower_statistics(self, data):
        """Analyse les donn√©es quotidiennes de followers de l'API et extrait les statistiques pertinentes"""
        daily_stats = []
        
        # S'assurer que les donn√©es sont valides
        if not data or 'elements' not in data or len(data['elements']) == 0:
            print("   Aucune donn√©e de statistiques quotidiennes de followers valide trouv√©e.")
            return daily_stats
        
        # Parcourir chaque √©l√©ment (un par jour)
        for element in data['elements']:
            # Ignorer les √©l√©ments sans plage de temps ou statistiques de followers
            if 'timeRange' not in element or 'followerGains' not in element:
                continue
            
            time_range = element['timeRange']
            follower_gains = element['followerGains']
            
            # Convertir le timestamp en date lisible
            # Le timestamp est en millisecondes depuis l'√©poque
            start_date = datetime.fromtimestamp(time_range['start'] / 1000)
            date_str = start_date.strftime('%Y-%m-%d')
            
            # Extraire les gains de followers
            organic_follower_gain = follower_gains.get('organicFollowerGain', 0)
            paid_follower_gain = follower_gains.get('paidFollowerGain', 0)
            
            # Stocker les donn√©es du jour
            day_stats = {
                'date': date_str,
                'organic_follower_gain': organic_follower_gain,
                'paid_follower_gain': paid_follower_gain,
                'total_follower_gain': organic_follower_gain + paid_follower_gain
            }
            
            daily_stats.append(day_stats)
        
        # Trier les statistiques par date (plus ancien au plus r√©cent)
        daily_stats.sort(key=lambda x: x['date'], reverse=False)
        
        return daily_stats


class LinkedInShareStatisticsTracker:
    """Classe pour suivre les statistiques quotidiennes des partages LinkedIn"""
    
    def __init__(self, access_token, organization_id, days_history=365):
        """Initialise le tracker avec le token d'acc√®s et l'ID de l'organisation"""
        self.access_token = access_token
        self.organization_id = organization_id
        self.days_history = min(days_history, 365)  # Maximum 12 mois selon la documentation
        self.base_url = "https://api.linkedin.com/v2"
        
    def get_headers(self):
        """Retourne les en-t√™tes pour les requ√™tes API"""
        return {
            "Authorization": f"Bearer {self.access_token}",
            "X-Restli-Protocol-Version": "2.0.0",
            "LinkedIn-Version": "202312",
            "Content-Type": "application/json"
        }
    
    def get_daily_share_statistics(self):
        """Obtient les statistiques quotidiennes des partages pour l'organisation"""
        # Encoder l'URN de l'organisation
        organization_urn = f"urn:li:organization:{self.organization_id}"
        encoded_urn = urllib.parse.quote(organization_urn)
        
        # Calculer les timestamps (millisecondes depuis l'√©poque)
        end_date = datetime.now()
        start_date = end_date - timedelta(days=self.days_history)
        
        # Convertir en millisecondes depuis l'√©poque
        start_timestamp = int(start_date.timestamp() * 1000)
        end_timestamp = int(end_date.timestamp() * 1000)
        
        # Construire l'URL avec le format RESTli 2.0
        url = (f"{self.base_url}/organizationalEntityShareStatistics?q=organizationalEntity&"
               f"organizationalEntity={encoded_urn}&"
               f"timeIntervals=(timeRange:(start:{start_timestamp},end:{end_timestamp}),"
               f"timeGranularityType:DAY)")
        
        # Effectuer la requ√™te avec gestion des erreurs et retry
        max_retries = 3
        retry_delay = 2  # secondes
        
        for attempt in range(max_retries):
            try:
                response = requests.get(url, headers=self.get_headers())
                
                if response.status_code == 200:
                    data = response.json()
                    print(f"   Donn√©es de statistiques quotidiennes des partages r√©cup√©r√©es avec succ√®s")
                    return data
                    
                elif response.status_code == 429:
                    # Rate limit, attendre avant de r√©essayer
                    print(f"   Rate limit atteint, attente de {retry_delay} secondes...")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # Backoff exponentiel
                else:
                    print(f"   Erreur API: {response.status_code} - {response.text}")
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    
            except Exception as e:
                print(f"   Exception lors de la requ√™te: {e}")
                time.sleep(retry_delay)
                retry_delay *= 2
        
        print("   √âchec apr√®s plusieurs tentatives pour obtenir les statistiques quotidiennes des partages.")
        return None
    
    def parse_daily_share_statistics(self, data):
        """Analyse les donn√©es quotidiennes de partages de l'API et extrait les statistiques pertinentes"""
        daily_stats = []
        
        # S'assurer que les donn√©es sont valides
        if not data or 'elements' not in data or len(data['elements']) == 0:
            print("   Aucune donn√©e de statistiques quotidiennes de partages valide trouv√©e.")
            return daily_stats
        
        # Parcourir chaque √©l√©ment (un par jour)
        for element in data['elements']:
            # Ignorer les √©l√©ments sans plage de temps ou statistiques
            if 'timeRange' not in element or 'totalShareStatistics' not in element:
                continue
            
            time_range = element['timeRange']
            share_stats = element['totalShareStatistics']
            
            # Convertir le timestamp en date lisible
            # Le timestamp est en millisecondes depuis l'√©poque
            start_date = datetime.fromtimestamp(time_range['start'] / 1000)
            date_str = start_date.strftime('%Y-%m-%d')
            
            # Extraire les m√©triques des partages
            click_count = share_stats.get('clickCount', 0)
            engagement = share_stats.get('engagement', 0)
            like_count = share_stats.get('likeCount', 0)
            comment_count = share_stats.get('commentCount', 0)
            share_count = share_stats.get('shareCount', 0)
            impression_count = share_stats.get('impressionCount', 0)
            unique_impressions_count = share_stats.get('uniqueImpressionsCount', 0)
            
            # Extraire les m√©triques additionnelles si disponibles
            share_mentions_count = share_stats.get('shareMentionsCount', 0)
            comment_mentions_count = share_stats.get('commentMentionsCount', 0)
            
            # Stocker les donn√©es du jour
            day_stats = {
                'date': date_str,
                'click_count': click_count,
                'engagement': engagement,
                'like_count': like_count,
                'comment_count': comment_count,
                'share_count': share_count,
                'impression_count': impression_count,
                'unique_impressions_count': unique_impressions_count,
                'share_mentions_count': share_mentions_count,
                'comment_mentions_count': comment_mentions_count
            }
            
            daily_stats.append(day_stats)
        
        # Trier les statistiques par date (plus ancien au plus r√©cent)
        daily_stats.sort(key=lambda x: x['date'], reverse=False)
        
        return daily_stats


class GoogleSheetsExporter:
    """Classe pour exporter les donn√©es vers Google Sheets avec gestion optimis√©e des quotas"""
    
    def __init__(self, spreadsheet_name, credentials_path, admin_email="byteberry.analytics@gmail.com"):
        """Initialise l'exportateur avec le nom du spreadsheet et le chemin des credentials"""
        self.spreadsheet_name = spreadsheet_name
        self.credentials_path = credentials_path
        self.admin_email = admin_email
        self.client = None
        self.spreadsheet = None
        # D√©tecter si on est en mode automatis√©
        self.is_automated = os.getenv('AUTOMATED_MODE', 'false').lower() == 'true'
        
    def connect(self):
        """√âtablit la connexion avec Google Sheets API"""
        try:
            scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
            creds = ServiceAccountCredentials.from_json_keyfile_name(str(self.credentials_path), scope)
            self.client = gspread.authorize(creds)
            
            # V√©rifier si le spreadsheet existe d√©j√†, sinon le cr√©er
            try:
                self.spreadsheet = self.client.open(self.spreadsheet_name)
                print(f"   Spreadsheet existant trouv√©: {self.spreadsheet_name}")
            except gspread.exceptions.SpreadsheetNotFound:
                self.spreadsheet = self.client.create(self.spreadsheet_name)
                print(f"   Nouveau spreadsheet cr√©√©: {self.spreadsheet_name}")
                
                # Donner l'acc√®s en √©dition √† l'adresse e-mail sp√©cifi√©e
                self.spreadsheet.share(self.admin_email, perm_type="user", role="writer")
                print(f"   Acc√®s en √©dition accord√© √† {self.admin_email}")
            
            # V√©rifier si Sheet1 existe et le renommer en "Statistiques quotidiennes"
            try:
                sheet1 = self.spreadsheet.worksheet("Sheet1")
                sheet1.update_title("Statistiques quotidiennes")
                print("   Feuille 'Sheet1' renomm√©e en 'Statistiques quotidiennes'")
            except gspread.exceptions.WorksheetNotFound:
                pass  # Sheet1 n'existe pas, pas besoin de la renommer
            
            return True
        except Exception as e:
            print(f"   Erreur de connexion √† Google Sheets: {e}")
            return False
    
    def ensure_admin_access(self):
        """V√©rifie et garantit que l'admin a toujours acc√®s au document"""
        try:
            # R√©cup√©rer les permissions actuelles
            permissions = self.spreadsheet.list_permissions()
            
            # V√©rifier si l'email admin est d√©j√† dans les permissions
            admin_has_access = False
            for permission in permissions:
                if 'emailAddress' in permission and permission['emailAddress'] == self.admin_email:
                    admin_has_access = True
                    # V√©rifier si le r√¥le est au moins "writer"
                    if permission.get('role') not in ['writer', 'owner']:
                        # Mettre √† jour le r√¥le si n√©cessaire
                        self.spreadsheet.share(self.admin_email, perm_type="user", role="writer")
                        print(f"   R√¥le mis √† jour pour {self.admin_email} (writer)")
                    break
            
            # Si l'admin n'a pas encore acc√®s, lui donner
            if not admin_has_access:
                self.spreadsheet.share(self.admin_email, perm_type="user", role="writer")
                print(f"   Acc√®s en √©dition accord√© √† {self.admin_email}")
                
        except Exception as e:
            print(f"   Erreur lors de la v√©rification des permissions: {e}")
    
    def wait_with_backoff(self, base_delay=2, max_delay=120, factor=2, jitter=0.2):
        """Impl√©mente une attente avec backoff exponentiel et jitter al√©atoire"""
        delay = base_delay + random.uniform(0, jitter * base_delay)
        time.sleep(delay)
        # Retourne le prochain d√©lai √† utiliser en cas de nouvelle erreur
        next_delay = min(delay * factor, max_delay)
        return next_delay
    
    def api_request_with_retry(self, api_func, *args, max_retries=6, initial_delay=2, **kwargs):
        """Ex√©cute une requ√™te API avec retry et backoff exponentiel en cas d'erreur"""
        delay = initial_delay
        for attempt in range(max_retries):
            try:
                return api_func(*args, **kwargs)
            except gspread.exceptions.APIError as e:
                if "429" in str(e) or "Quota exceeded" in str(e):  # Quota exceeded
                    if attempt == max_retries - 1:
                        print(f"   ‚ùå Quota API d√©finitivement d√©pass√© apr√®s {max_retries} tentatives")
                        raise Exception(f"Quota API d√©pass√© - abandon apr√®s {max_retries} tentatives")
                    
                    print(f"   ‚ö†Ô∏è  Quota API d√©pass√© (tentative {attempt+1}/{max_retries}). Attente de {delay:.1f}s...")
                    delay = self.wait_with_backoff(base_delay=delay)
                else:
                    # Autres erreurs d'API, retry mais avec un log diff√©rent
                    if attempt == max_retries - 1:
                        print(f"   ‚ùå Erreur API persistante apr√®s {max_retries} tentatives: {e}")
                        raise
                    print(f"   ‚ö†Ô∏è  Erreur API (tentative {attempt+1}/{max_retries}): {e}. Attente de {delay:.1f}s...")
                    delay = self.wait_with_backoff(base_delay=delay)
            except Exception as e:
                # Autres exceptions g√©n√©riques, ne pas retry automatiquement
                print(f"   ‚ùå Erreur non r√©cup√©rable: {e}")
                raise
        
        # Si on arrive ici, c'est qu'on a √©puis√© toutes les tentatives
        raise Exception(f"Nombre maximum de tentatives ({max_retries}) atteint. √âchec de l'op√©ration.")
    
    def save_progress(self, processed_dates, org_id):
        """Sauvegarde l'√©tat de progression dans un fichier pour pouvoir reprendre plus tard"""
        try:
            progress_file = f'linkedin_stats_progress_{org_id}.json'
            with open(progress_file, 'w') as f:
                json.dump({"processed_dates": list(processed_dates)}, f)
            print(f"   üìÑ Progression sauvegard√©e: {len(processed_dates)} dates trait√©es")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur lors de la sauvegarde de la progression: {e}")
    
    def load_progress(self, org_id):
        """Charge l'√©tat de progression depuis un fichier"""
        try:
            progress_file = f'linkedin_stats_progress_{org_id}.json'
            if os.path.exists(progress_file):
                with open(progress_file, 'r') as f:
                    data = json.load(f)
                processed_dates = set(data.get("processed_dates", []))
                print(f"   üìã Progression charg√©e: {len(processed_dates)} dates d√©j√† trait√©es")
                return processed_dates
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur lors du chargement de la progression (reprise √† z√©ro): {e}")
        return set()
    
    def merge_all_stats(self, page_stats, follower_stats, share_stats):
        """Combine les statistiques de pages, followers et partages par date"""
        merged_stats = {}
        
        # D'abord, ajouter toutes les statistiques de pages par date
        for stat in page_stats:
            date = stat['date']
            merged_stats[date] = stat.copy()
            
            # Initialiser les champs de followers √† 0
            merged_stats[date]['organic_follower_gain'] = 0
            merged_stats[date]['paid_follower_gain'] = 0
            merged_stats[date]['total_follower_gain'] = 0
            
            # Initialiser les champs de partages √† 0
            merged_stats[date]['click_count'] = 0
            merged_stats[date]['engagement'] = 0
            merged_stats[date]['like_count'] = 0
            merged_stats[date]['comment_count'] = 0
            merged_stats[date]['share_count'] = 0
            merged_stats[date]['impression_count'] = 0
            merged_stats[date]['unique_impressions_count'] = 0
            merged_stats[date]['share_mentions_count'] = 0
            merged_stats[date]['comment_mentions_count'] = 0
        
        # Ensuite, ajouter ou mettre √† jour avec les statistiques de followers
        for stat in follower_stats:
            date = stat['date']
            if date in merged_stats:
                # Mettre √† jour une entr√©e existante
                merged_stats[date]['organic_follower_gain'] = stat['organic_follower_gain']
                merged_stats[date]['paid_follower_gain'] = stat['paid_follower_gain']
                merged_stats[date]['total_follower_gain'] = stat['total_follower_gain']
            else:
                # Cr√©er une nouvelle entr√©e (avec des vues √† 0)
                new_entry = {
                    'date': date,
                    'total_views': 0,
                    'unique_views': 0,
                    'desktop_views': 0,
                    'unique_desktop_views': 0,
                    'mobile_views': 0,
                    'unique_mobile_views': 0,
                    'overview_views': 0,
                    'unique_overview_views': 0,
                    'about_views': 0,
                    'unique_about_views': 0,
                    'people_views': 0,
                    'unique_people_views': 0,
                    'jobs_views': 0,
                    'unique_jobs_views': 0,
                    'careers_views': 0,
                    'unique_careers_views': 0,
                    'desktop_careers_views': 0,
                    'desktop_jobs_views': 0,
                    'desktop_overview_views': 0,
                    'desktop_life_at_views': 0,
                    'mobile_careers_views': 0,
                    'mobile_jobs_views': 0,
                    'mobile_overview_views': 0,
                    'mobile_life_at_views': 0,
                    'life_at_views': 0,
                    'unique_life_at_views': 0,
                    'desktop_button_clicks': 0,
                    'mobile_button_clicks': 0,
                    'total_button_clicks': 0,
                    'organic_follower_gain': stat['organic_follower_gain'],
                    'paid_follower_gain': stat['paid_follower_gain'],
                    'total_follower_gain': stat['total_follower_gain'],
                    'click_count': 0,
                    'engagement': 0,
                    'like_count': 0,
                    'comment_count': 0,
                    'share_count': 0,
                    'impression_count': 0,
                    'unique_impressions_count': 0,
                    'share_mentions_count': 0,
                    'comment_mentions_count': 0
                }
                merged_stats[date] = new_entry
        
        # Finalement, ajouter ou mettre √† jour avec les statistiques de partages
        for stat in share_stats:
            date = stat['date']
            if date in merged_stats:
                # Mettre √† jour une entr√©e existante
                merged_stats[date]['click_count'] = stat['click_count']
                merged_stats[date]['engagement'] = stat['engagement']
                merged_stats[date]['like_count'] = stat['like_count']
                merged_stats[date]['comment_count'] = stat['comment_count']
                merged_stats[date]['share_count'] = stat['share_count']
                merged_stats[date]['impression_count'] = stat['impression_count']
                merged_stats[date]['unique_impressions_count'] = stat['unique_impressions_count']
                # Ajout des nouveaux champs
                merged_stats[date]['share_mentions_count'] = stat.get('share_mentions_count', 0)
                merged_stats[date]['comment_mentions_count'] = stat.get('comment_mentions_count', 0)
            else:
                # Cr√©er une nouvelle entr√©e (avec des vues et followers √† 0)
                new_entry = {
                    'date': date,
                    'total_views': 0,
                    'unique_views': 0,
                    'desktop_views': 0,
                    'unique_desktop_views': 0,
                    'mobile_views': 0,
                    'unique_mobile_views': 0,
                    'overview_views': 0,
                    'unique_overview_views': 0,
                    'about_views': 0,
                    'unique_about_views': 0,
                    'people_views': 0,
                    'unique_people_views': 0,
                    'jobs_views': 0,
                    'unique_jobs_views': 0,
                    'careers_views': 0,
                    'unique_careers_views': 0,
                    'desktop_careers_views': 0,
                    'desktop_jobs_views': 0,
                    'desktop_overview_views': 0,
                    'desktop_life_at_views': 0,
                    'mobile_careers_views': 0,
                    'mobile_jobs_views': 0,
                    'mobile_overview_views': 0,
                    'mobile_life_at_views': 0,
                    'life_at_views': 0,
                    'unique_life_at_views': 0,
                    'desktop_button_clicks': 0,
                    'mobile_button_clicks': 0,
                    'total_button_clicks': 0,
                    'organic_follower_gain': 0,
                    'paid_follower_gain': 0,
                    'total_follower_gain': 0,
                    'click_count': stat['click_count'],
                    'engagement': stat['engagement'],
                    'like_count': stat['like_count'],
                    'comment_count': stat['comment_count'],
                    'share_count': stat['share_count'],
                    'impression_count': stat['impression_count'],
                    'unique_impressions_count': stat['unique_impressions_count'],
                    'share_mentions_count': stat.get('share_mentions_count', 0),
                    'comment_mentions_count': stat.get('comment_mentions_count', 0)
                }
                merged_stats[date] = new_entry
        
        # Convertir le dictionnaire en liste et trier par date
        merged_list = list(merged_stats.values())
        merged_list.sort(key=lambda x: x['date'], reverse=False)
        
        return merged_list
    
    def format_columns_optimized(self, sheet, headers):
        """Applique le formatage optimis√© avec moins de requ√™tes API"""
        try:
            print("   üé® Application du formatage optimis√© des colonnes...")
            
            # 1. Formatage global en une seule requ√™te pour toutes les colonnes de donn√©es
            last_col = get_column_letter(len(headers) - 1)
            
            # Formatage de la colonne date (A)
            try:
                self.api_request_with_retry(
                    sheet.format, 
                    "A:A", 
                    {
                        "numberFormat": {
                            "type": "DATE",
                            "pattern": "yyyy-mm-dd"
                        }
                    }
                )
                print("   ‚úÖ Formatage date appliqu√© √† la colonne A")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Impossible de formater la colonne date: {e}")
            
            # Formatage num√©rique pour toutes les colonnes de donn√©es sauf la colonne engagement
            try:
                engagement_col_index = None
                try:
                    engagement_col_index = headers.index("Taux d'engagement (engagement)")
                except ValueError:
                    pass
                
                # Formater toutes les colonnes num√©riques en une fois
                numeric_range = f"B:{last_col}"
                self.api_request_with_retry(
                    sheet.format, 
                    numeric_range, 
                    {
                        "numberFormat": {
                            "type": "NUMBER",
                            "pattern": "#,##0"
                        }
                    }
                )
                print("   ‚úÖ Formatage num√©rique global appliqu√©")
                
                # Formatage sp√©cial pour la colonne engagement si elle existe
                if engagement_col_index is not None:
                    engagement_col_letter = get_column_letter(engagement_col_index)
                    try:
                        self.api_request_with_retry(
                            sheet.format, 
                            f"{engagement_col_letter}:{engagement_col_letter}", 
                            {
                                "numberFormat": {
                                    "type": "PERCENT",
                                    "pattern": "0.00%"
                                }
                            }
                        )
                        print(f"   ‚úÖ Formatage pourcentage appliqu√© √† la colonne {engagement_col_letter}")
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  Impossible de formater la colonne engagement: {e}")
                        
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Impossible d'appliquer le formatage num√©rique global: {e}")
            
            # 2. Formatage des en-t√™tes
            try:
                header_range = f'A1:{last_col}1'
                self.api_request_with_retry(
                    sheet.format, 
                    header_range, 
                    {
                        "textFormat": {"bold": True},
                        "backgroundColor": {"red": 0.9, "green": 0.9, "blue": 0.9}
                    }
                )
                print("   ‚úÖ Formatage des en-t√™tes appliqu√©")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Impossible de formater les en-t√™tes: {e}")
                
            print("   ‚úÖ Formatage optimis√© termin√©")
                
        except Exception as e:
            print(f"   ‚ùå Erreur lors du formatage optimis√© des colonnes: {e}")
    
    def update_daily_stats_sheet(self, combined_stats, org_id):
        """Met √† jour la feuille des statistiques quotidiennes avec gestion optimis√©e des quotas API"""
        try:
            # D√©tecter si on est en mode automatis√©
            if self.is_automated:
                print("   ü§ñ Mode automatis√© d√©tect√© - Force la mise √† jour des donn√©es r√©centes")
            
            # Charger l'√©tat de progression ant√©rieur si disponible
            processed_dates = self.load_progress(org_id)
            
            # V√©rifier si la feuille existe ou la cr√©er
            try:
                sheet = self.spreadsheet.worksheet("Statistiques quotidiennes")
                print("   üìä Feuille 'Statistiques quotidiennes' trouv√©e")
            except gspread.exceptions.WorksheetNotFound:
                try:
                    sheet = self.spreadsheet.worksheet("Sheet1")
                    self.api_request_with_retry(sheet.update_title, "Statistiques quotidiennes")
                    print("   üìä Feuille par d√©faut 'Sheet1' renomm√©e en 'Statistiques quotidiennes'")
                except gspread.exceptions.WorksheetNotFound:
                    sheet = self.api_request_with_retry(
                        self.spreadsheet.add_worksheet, 
                        title="Statistiques quotidiennes", 
                        rows=500, 
                        cols=50  # Augmentation du nombre de colonnes pour les nouvelles m√©triques
                    )
                    print("   üìä Nouvelle feuille 'Statistiques quotidiennes' cr√©√©e")
            
            # R√©cup√©rer les donn√©es existantes avec des retries si n√©cessaire
            existing_data = self.api_request_with_retry(sheet.get_all_values)
            has_headers = len(existing_data) > 0
            
            # Cr√©er les en-t√™tes si n√©cessaire
            headers = [
                "Date",
                
                # --- Vues de page ---
                # Vues g√©n√©rales
                "Vues totales (allPageViews)", 
                "Vues uniques (uniquePageViews)", 
                
                # Vues par appareil
                "Vues Desktop (allDesktopPageViews)", 
                "Vues Desktop uniques (uniqueDesktopPageViews)", 
                "Vues Mobile (allMobilePageViews)",
                "Vues Mobile uniques (uniqueMobilePageViews)",
                
                # Vues par section - Aper√ßu
                "Vues Accueil (overviewPageViews)", 
                "Vues Accueil uniques (uniqueOverviewPageViews)",
                "Vues Accueil Desktop (desktopOverviewPageViews)",
                "Vues Accueil Mobile (mobileOverviewPageViews)",
                
                # Vues par section - √Ä propos
                "Vues √Ä propos (aboutPageViews)", 
                "Vues √Ä propos uniques (uniqueAboutPageViews)",
                
                # Vues par section - Personnes
                "Vues Personnes (peoplePageViews)", 
                "Vues Personnes uniques (uniquePeoplePageViews)",
                
                # Vues par section - Emplois
                "Vues Emplois (jobsPageViews)", 
                "Vues Emplois uniques (uniqueJobsPageViews)",
                "Vues Emplois Desktop (desktopJobsPageViews)",
                "Vues Emplois Mobile (mobileJobsPageViews)",
                
                # Vues par section - Carri√®res
                "Vues Carri√®res (careersPageViews)", 
                "Vues Carri√®res uniques (uniqueCareersPageViews)",
                "Vues Carri√®res Desktop (desktopCareersPageViews)",
                "Vues Carri√®res Mobile (mobileCareersPageViews)",
                
                # Vues par section - Vie en entreprise
                "Vues Vie en entreprise (lifeAtPageViews)",
                "Vues Vie en entreprise uniques (uniqueLifeAtPageViews)",
                "Vues Vie en entreprise Desktop (desktopLifeAtPageViews)",
                "Vues Vie en entreprise Mobile (mobileLifeAtPageViews)",
                
                # Clics sur boutons
                "Clics sur boutons Desktop (desktopCustomButtonClickCounts)",
                "Clics sur boutons Mobile (mobileCustomButtonClickCounts)",
                "Total clics sur boutons",
                
                # --- Followers ---
                "Nouveaux followers organiques (organicFollowerGain)",
                "Nouveaux followers payants (paidFollowerGain)",
                "Total nouveaux followers (totalFollowerGain)",
                
                # --- Partages ---
                "Nombre de clics (clickCount)",
                "Taux d'engagement (engagement)",
                "Nombre de J'aime (likeCount)",
                "Nombre de commentaires (commentCount)",
                "Nombre de partages (shareCount)",
                "Mentions dans partages (shareMentionsCount)",
                "Mentions dans commentaires (commentMentionsCount)",
                "Nombre d'impressions (impressionCount)",
                "Nombre d'impressions uniques (uniqueImpressionsCount)"
            ]
            
            # S'assurer que la premi√®re ligne contient les en-t√™tes
            if not has_headers or len(existing_data) == 0 or len(existing_data[0]) == 0 or existing_data[0][0] == '':
                print("   üìù Ajout des en-t√™tes en premi√®re ligne")
                # Ajouter les en-t√™tes en une seule op√©ration avec retry
                self.api_request_with_retry(sheet.update, values=[headers], range_name='A1')
                
                # Appliquer le formatage des en-t√™tes
                self.format_columns_optimized(sheet, headers)
                
                # Pour le cas o√π les en-t√™tes viennent d'√™tre ajout√©s
                existing_data = self.api_request_with_retry(sheet.get_all_values)
                if len(existing_data) <= 1:
                    existing_dates = []
                    existing_dates_dict = {}
                else:
                    existing_dates = [row[0] for row in existing_data[1:]]
                    existing_dates_dict = {row[0]: idx + 2 for idx, row in enumerate(existing_data[1:])}
            else:
                # R√©cup√©rer les dates existantes (colonne A) avec leur position
                if len(existing_data) > 1:  # Si des donn√©es existent au-del√† des en-t√™tes
                    existing_dates = [row[0] for row in existing_data[1:]]
                    # Cr√©er un dictionnaire date -> num√©ro de ligne
                    existing_dates_dict = {row[0]: idx + 2 for idx, row in enumerate(existing_data[1:])}
                else:
                    existing_dates = []
                    existing_dates_dict = {}
                    
                # V√©rifier si les en-t√™tes correspondent √† ce qu'on attend
                if len(existing_data[0]) < len(headers):
                    print("   üìù Mise √† jour des en-t√™tes pour correspondre au format attendu")
                    self.api_request_with_retry(sheet.update, values=[headers], range_name='A1')
                    
                    # Appliquer le formatage optimis√©
                    self.format_columns_optimized(sheet, headers)
            
            # Obtenir la date d'aujourd'hui et d'hier
            today = datetime.now().strftime('%Y-%m-%d')
            yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
            
            print(f"   üìÖ Date du jour: {today}")
            print(f"   üìÖ Date d'hier: {yesterday}")
            
            # Cr√©er un dictionnaire des dates des nouvelles donn√©es
            new_data_dict = {stat['date']: stat for stat in combined_stats}
            
            # Pr√©parer les mises √† jour en lots
            updates_by_date = {}
            new_rows = []
            
            # Identifier les dates √† traiter
            dates_to_process = set()
            
            # En mode automatis√©, forcer la mise √† jour des 7 derniers jours
            if self.is_automated:
                print("   üîÑ Mode automatis√©: Force la mise √† jour des 7 derniers jours")
                for i in range(7):
                    date_to_update = (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')
                    if date_to_update in new_data_dict:
                        dates_to_process.add(date_to_update)
                        print(f"   üìÖ Ajout forc√©: {date_to_update}")
            else:
                # Mode manuel : comportement existant
                # 1. Toujours traiter aujourd'hui et hier s'ils sont dans les donn√©es
                if today in new_data_dict:
                    dates_to_process.add(today)
                if yesterday in new_data_dict:
                    dates_to_process.add(yesterday)
            
            # 2. Ajouter les dates qui n'existent pas encore dans le sheet
            for date in new_data_dict:
                if date not in existing_dates_dict:
                    dates_to_process.add(date)
            
            print(f"   üìä Dates √† traiter: {len(dates_to_process)} sur {len(new_data_dict)}")
            
            # Log d√©taill√© en mode automatis√©
            if self.is_automated and len(dates_to_process) < 20:
                print(f"   üìã Dates forc√©es: {sorted(list(dates_to_process), reverse=True)}")
            
            # Traiter uniquement les dates identifi√©es
            for date in dates_to_process:
                if date not in new_data_dict:
                    continue
                    
                day_stats = new_data_dict[date]
                
                # Pr√©paration des donn√©es de ligne
                row_data = [
                    date,
                    
                    # Vues g√©n√©rales
                    day_stats['total_views'],
                    day_stats['unique_views'],
                    
                    # Vues par appareil
                    day_stats['desktop_views'],
                    day_stats['unique_desktop_views'],
                    day_stats['mobile_views'],
                    day_stats['unique_mobile_views'],
                    
                    # Vues par section - Aper√ßu
                    day_stats['overview_views'],
                    day_stats['unique_overview_views'],
                    day_stats['desktop_overview_views'],
                    day_stats['mobile_overview_views'],
                    
                    # Vues par section - √Ä propos
                    day_stats['about_views'],
                    day_stats['unique_about_views'],
                    
                    # Vues par section - Personnes
                    day_stats['people_views'],
                    day_stats['unique_people_views'],
                    
                    # Vues par section - Emplois
                    day_stats['jobs_views'],
                    day_stats['unique_jobs_views'],
                    day_stats['desktop_jobs_views'],
                    day_stats['mobile_jobs_views'],
                    
                    # Vues par section - Carri√®res
                    day_stats['careers_views'],
                    day_stats['unique_careers_views'],
                    day_stats['desktop_careers_views'],
                    day_stats['mobile_careers_views'],
                    
                    # Vues par section - Vie en entreprise
                    day_stats['life_at_views'],
                    day_stats['unique_life_at_views'],
                    day_stats['desktop_life_at_views'],
                    day_stats['mobile_life_at_views'],
                    
                    # Clics sur boutons
                    day_stats['desktop_button_clicks'],
                    day_stats['mobile_button_clicks'],
                    day_stats['total_button_clicks'],
                    
                    # Followers
                    day_stats['organic_follower_gain'],
                    day_stats['paid_follower_gain'],
                    day_stats['total_follower_gain'],
                    
                    # Partages
                    day_stats['click_count'],
                    day_stats['engagement'],
                    day_stats['like_count'],
                    day_stats['comment_count'],
                    day_stats['share_count'],
                    day_stats.get('share_mentions_count', 0),
                    day_stats.get('comment_mentions_count', 0),
                    day_stats['impression_count'],
                    day_stats['unique_impressions_count']
                ]
                
                # V√©rifier si cette date existe d√©j√†
                if date in existing_dates_dict:
                    # En mode automatis√©, toujours mettre √† jour
                    # En mode manuel, mettre √† jour uniquement les 3 derniers jours
                    recent_dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(3)]
                    if self.is_automated or date in recent_dates:
                        row_index = existing_dates_dict[date]
                        updates_by_date[row_index] = row_data
                        print(f"   üîÑ {'[AUTO]' if self.is_automated else '[MANUAL]'} Mise √† jour: {date}")
                else:
                    # Nouvelle date √† ajouter
                    new_rows.append(row_data)
                    print(f"   ‚ûï Nouvelle date: {date}")
            
            # 1. Ajouter de nouvelles lignes en lots avec d√©lais plus longs
            if new_rows:
                print(f"   üìä Ajout de {len(new_rows)} nouvelles dates")
                # Trier les nouvelles lignes par date avant de les ajouter
                new_rows.sort(key=lambda x: x[0])
                
                # R√©duire √† des lots plus petits pour √©viter les quotas
                batch_size = 8  # R√©duction de la taille des lots
                for i in range(0, len(new_rows), batch_size):
                    batch = new_rows[i:i+batch_size]
                    # S'assurer d'avoir au moins une ligne d'en-t√™te avant d'ajouter
                    first_empty_row = len(existing_data) + 1
                    if first_empty_row == 1:  # Si aucune donn√©e n'existe encore
                        first_empty_row = 2  # Commencer √† la ligne 2 (apr√®s les en-t√™tes)
                    
                    try:
                        # Calculer le range pour l'ajout
                        last_col = get_column_letter(len(headers) - 1)
                        range_name = f'A{first_empty_row}:{last_col}{first_empty_row + len(batch) - 1}'
                        self.api_request_with_retry(sheet.update, values=batch, range_name=range_name)
                        
                        # Mettre √† jour les variables pour la prochaine it√©ration
                        existing_data.extend(batch)
                        
                        print(f"   ‚úÖ Lot {i//batch_size + 1}/{(len(new_rows)-1)//batch_size + 1} ajout√© ({len(batch)} lignes)")
                        
                        # Ajouter les dates trait√©es √† la liste de progression
                        for row in batch:
                            processed_dates.add(row[0])  # Ajouter la date (premi√®re colonne)
                        
                        # Sauvegarder la progression r√©guli√®rement
                        if i % (batch_size * 2) == 0:
                            self.save_progress(processed_dates, org_id)
                        
                        # Attendre plus longtemps entre les lots pour √©viter les quotas
                        time.sleep(8)  # Augmentation du d√©lai
                    except Exception as e:
                        print(f"   ‚ùå Erreur lors de l'ajout du lot {i//batch_size + 1}: {e}")
                        # Sauvegarder l'√©tat actuel avant de continuer
                        self.save_progress(processed_dates, org_id)
                        # Continue avec le lot suivant
                        continue
            
            # 2. Mettre √† jour les lignes existantes
            if updates_by_date:
                print(f"   üîÑ Mise √† jour de {len(updates_by_date)} dates")
                # Pour √©viter les timeouts, on met √† jour ligne par ligne avec pauses plus longues
                for row_idx, row_data in updates_by_date.items():
                    try:
                        last_col = get_column_letter(len(headers) - 1)
                        range_name = f'A{row_idx}:{last_col}{row_idx}'
                        self.api_request_with_retry(sheet.update, values=[row_data], range_name=range_name)
                        processed_dates.add(row_data[0])  # Ajouter la date
                        print(f"   ‚úÖ Mise √† jour effectu√©e pour {row_data[0]} (ligne {row_idx})")
                        time.sleep(3)  # Pause plus longue entre chaque mise √† jour
                    except Exception as e:
                        print(f"   ‚ùå Erreur lors de la mise √† jour de la ligne {row_idx}: {e}")
                        self.save_progress(processed_dates, org_id)
                        continue
            
            # 3. Tri des donn√©es - TOUJOURS actif pour le multi-organisation
            print("   üîÑ Tri des donn√©es par date (du plus ancien au plus r√©cent)...")
            try:
                # R√©cup√©rer toutes les donn√©es actualis√©es
                updated_data = self.api_request_with_retry(sheet.get_all_values)
                if len(updated_data) > 1:
                    # Exclure la ligne d'en-t√™te pour le tri
                    last_col = get_column_letter(len(headers) - 1)
                    data_range = f'A2:{last_col}{len(updated_data)}'
                    try:
                        self.api_request_with_retry(sheet.sort, (1, 'asc'), range=data_range)
                        print("   ‚úÖ Tri termin√©")
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  Impossible de trier les donn√©es: {e}")
                        print("   Le tri sera ignor√© pour cette ex√©cution")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Erreur lors du tri des donn√©es: {e}")
            
            # Sauvegarder la progression finale
            self.save_progress(processed_dates, org_id)
            
            # R√©sum√©
            print(f"\n   üìä R√©sum√© des modifications:")
            print(f"   - Nouvelles lignes ajout√©es: {len(new_rows)}")
            print(f"   - Lignes mises √† jour: {len(updates_by_date)}")
            print(f"   - Lignes ignor√©es (d√©j√† √† jour): {len(combined_stats) - len(dates_to_process)}")
            
            # Supprimer le fichier de progression si tout s'est bien pass√©
            progress_file = f'linkedin_stats_progress_{org_id}.json'
            if os.path.exists(progress_file):
                os.remove(progress_file)
                print("   üóëÔ∏è  Fichier de progression supprim√© (traitement termin√© avec succ√®s)")
                
            return sheet
        except Exception as e:
            print(f"   ‚ùå Erreur lors de la mise √† jour de la feuille des statistiques quotidiennes: {e}")
            return None
    
    def add_combined_statistics(self, page_stats, follower_stats, share_stats, org_id):
        """Ajoute les statistiques quotidiennes combin√©es (pages, followers et partages)"""
        if not self.connect():
            print("   ‚ùå Impossible de se connecter √† Google Sheets. V√©rifiez vos credentials.")
            return False
            
        # V√©rifier les permissions de partage pour s'assurer que l'admin a toujours acc√®s
        self.ensure_admin_access()
        
        # Combiner les statistiques
        combined_stats = self.merge_all_stats(page_stats, follower_stats, share_stats)
        
        # Mettre √† jour la feuille principale
        if not self.update_daily_stats_sheet(combined_stats, org_id):
            return False
        
        # URL du spreadsheet
        sheet_url = f"https://docs.google.com/spreadsheets/d/{self.spreadsheet.id}"
        print(f"   üìä URL du tableau: {sheet_url}")
        
        return True


def verify_token(access_token):
    """V√©rifie si le token d'acc√®s est valide"""
    headers = {
        "Authorization": f"Bearer {access_token}",
        "X-Restli-Protocol-Version": "2.0.0",
        "LinkedIn-Version": "202312"
    }
    
    url = "https://api.linkedin.com/v2/me"
    
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            return True, response.json()
        else:
            return False, f"Erreur {response.status_code}: {response.text}"
    except Exception as e:
        return False, str(e)


class MultiOrganizationTracker:
    """Gestionnaire pour plusieurs organisations LinkedIn"""
    
    def __init__(self, config_file='organizations_config.json'):
        """Initialise le tracker multi-organisations"""
        self.config_file = config_file
        self.organizations = self.load_organizations()
        self.access_token = os.getenv("LINKEDIN_ACCESS_TOKEN", "").strip("'")
        self.days_history = int(os.getenv("LINKEDIN_DAYS_HISTORY", "365"))
        self.admin_email = os.getenv("GOOGLE_ADMIN_EMAIL", "byteberry.analytics@gmail.com")
        # D√©tecter si on est en mode automatis√©
        self.is_automated = os.getenv('AUTOMATED_MODE', 'false').lower() == 'true'
        
    def load_organizations(self):
        """Charge la configuration des organisations depuis un fichier JSON"""
        try:
            # D'abord essayer de charger depuis un fichier local
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            
            # Sinon, charger depuis une variable d'environnement
            orgs_json = os.getenv("LINKEDIN_ORGANIZATIONS_CONFIG")
            if orgs_json:
                return json.loads(orgs_json)
            
            # Configuration par d√©faut
            print("Aucune configuration trouv√©e, utilisation de la configuration par d√©faut")
            return []
            
        except Exception as e:
            print(f"Erreur lors du chargement de la configuration: {e}")
            return []
    
    def get_sheet_id_for_org(self, org_id, org_name):
        """R√©cup√®re ou cr√©e l'ID du Google Sheet pour une organisation"""
        # Utiliser un mapping stock√© dans les variables d'environnement ou un fichier
        sheet_mapping_file = 'sheet_mapping.json'
        
        try:
            if os.path.exists(sheet_mapping_file):
                with open(sheet_mapping_file, 'r', encoding='utf-8') as f:
                    mapping = json.load(f)
            else:
                mapping = {}
            
            # Si l'organisation a d√©j√† un sheet ID, le retourner
            if org_id in mapping:
                return mapping[org_id]['sheet_id'], mapping[org_id]['sheet_name']
            
            # Sinon, utiliser le nom par d√©faut
            # Nettoyer le nom pour √©viter les caract√®res probl√©matiques
            clean_name = org_name.replace(' ', '_').replace('‚Ñ¢', '').replace('/', '_')
            sheet_name = f"LinkedIn_Stats_{clean_name}_{org_id}"
            
            # Stocker le mapping pour la prochaine fois
            mapping[org_id] = {
                'sheet_name': sheet_name,
                'sheet_id': None,  # Sera mis √† jour apr√®s cr√©ation
                'org_name': org_name
            }
            
            with open(sheet_mapping_file, 'w', encoding='utf-8') as f:
                json.dump(mapping, f, indent=2, ensure_ascii=False)
            
            return None, sheet_name
            
        except Exception as e:
            print(f"Erreur dans la gestion du mapping: {e}")
            clean_name = org_name.replace(' ', '_').replace('‚Ñ¢', '').replace('/', '_')
            sheet_name = f"LinkedIn_Stats_{clean_name}_{org_id}"
            return None, sheet_name
    
    def update_sheet_mapping(self, org_id, sheet_id):
        """Met √† jour le mapping avec l'ID du sheet cr√©√©"""
        sheet_mapping_file = 'sheet_mapping.json'
        
        try:
            if os.path.exists(sheet_mapping_file):
                with open(sheet_mapping_file, 'r', encoding='utf-8') as f:
                    mapping = json.load(f)
            else:
                mapping = {}
            
            if org_id in mapping:
                mapping[org_id]['sheet_id'] = sheet_id
                mapping[org_id]['sheet_url'] = f"https://docs.google.com/spreadsheets/d/{sheet_id}"
                
                with open(sheet_mapping_file, 'w', encoding='utf-8') as f:
                    json.dump(mapping, f, indent=2, ensure_ascii=False)
                    
        except Exception as e:
            print(f"Erreur lors de la mise √† jour du mapping: {e}")
    
    def process_all_organizations(self):
        """Traite toutes les organisations configur√©es"""
        if not self.access_token:
            print("‚ùå Erreur: LINKEDIN_ACCESS_TOKEN manquant")
            return False
        
        # Afficher le mode d'ex√©cution
        if self.is_automated:
            print("\nü§ñ === MODE AUTOMATIS√â ACTIV√â ===")
            print("Les 7 derniers jours seront forc√©ment mis √† jour pour chaque organisation")
        else:
            print("\nüë§ === MODE MANUEL ===")
            print("Seuls les 3 derniers jours seront mis √† jour pour les dates existantes")
        
        # V√©rifier le token une seule fois
        print("\n--- V√©rification du token ---")
        is_valid, result = verify_token(self.access_token)
        
        if not is_valid:
            print(f"‚ùå Token invalide: {result}")
            return False
        
        print("‚úÖ Token valide!")
        user_info = result
        print(f"   Utilisateur: {user_info.get('localizedFirstName', '')} {user_info.get('localizedLastName', '')}")
        
        # Traiter chaque organisation avec une pause plus longue entre chaque org
        results = []
        total_orgs = len(self.organizations)
        
        for idx, org in enumerate(self.organizations, 1):
            org_id = org['id']
            org_name = org['name']
            
            print(f"\n{'='*60}")
            print(f"[{idx}/{total_orgs}] Traitement de: {org_name}")
            print(f"ID: {org_id}")
            print(f"{'='*60}")
            
            try:
                success = self.process_single_organization(org_id, org_name)
                results.append({
                    'org_id': org_id,
                    'org_name': org_name,
                    'success': success,
                    'timestamp': datetime.now().isoformat()
                })
                
                # Pause plus longue entre les organisations pour √©viter les quotas
                if idx < total_orgs:  # Pas de pause apr√®s la derni√®re organisation
                    delay = 15 if self.is_automated else 30  # Pause plus courte en mode automatis√©
                    print(f"\n‚è±Ô∏è  Pause de {delay} secondes avant la prochaine organisation...")
                    time.sleep(delay)
                    
            except Exception as e:
                print(f"‚ùå Erreur lors du traitement de {org_name}: {e}")
                results.append({
                    'org_id': org_id,
                    'org_name': org_name,
                    'success': False,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
                
                # Pause m√™me en cas d'erreur
                if idx < total_orgs:
                    delay = 15 if self.is_automated else 30
                    print(f"\n‚è±Ô∏è  Pause de {delay} secondes avant la prochaine organisation...")
                    time.sleep(delay)
        
        # R√©sum√©
        print(f"\n{'='*60}")
        print("R√âSUM√â DU TRAITEMENT")
        print(f"{'='*60}")
        
        successful = sum(1 for r in results if r['success'])
        failed = len(results) - successful
        
        print(f"‚úÖ Organisations trait√©es avec succ√®s: {successful}/{total_orgs}")
        if failed > 0:
            print(f"‚ùå Organisations en √©chec: {failed}/{total_orgs}")
        
        if failed > 0:
            print("\nD√©tail des √©checs:")
            for r in results:
                if not r['success']:
                    error_msg = r.get('error', 'Erreur inconnue')
                    print(f"  - {r['org_name']}: {error_msg}")
        
        # Afficher les URLs des sheets cr√©√©s
        if successful > 0:
            print("\nüìä Google Sheets cr√©√©s/mis √† jour:")
            sheet_mapping_file = 'sheet_mapping.json'
            if os.path.exists(sheet_mapping_file):
                with open(sheet_mapping_file, 'r', encoding='utf-8') as f:
                    mapping = json.load(f)
                
                for r in results:
                    if r['success'] and r['org_id'] in mapping:
                        sheet_info = mapping[r['org_id']]
                        if sheet_info.get('sheet_id'):
                            print(f"  - {r['org_name']}:")
                            url = sheet_info.get('sheet_url') or f"https://docs.google.com/spreadsheets/d/{sheet_info['sheet_id']}"
                            print(f"    {url}")
        
        return successful > 0
    
    def process_single_organization(self, org_id, org_name):
        """Traite une organisation unique"""
        # Obtenir le nom du sheet pour cette organisation
        sheet_id, sheet_name = self.get_sheet_id_for_org(org_id, org_name)
        
        print(f"\nüìä Google Sheet: {sheet_name}")
        
        # 1. R√©cup√©ration des statistiques de pages
        print(f"\n1. R√©cup√©ration des statistiques de pages ({self.days_history} jours)...")
        page_tracker = LinkedInDailyPageStatisticsTracker(
            self.access_token, org_id, self.days_history, sheet_name
        )
        raw_page_stats = page_tracker.get_daily_page_statistics()
        
        if raw_page_stats:
            page_stats = page_tracker.parse_daily_page_statistics(raw_page_stats)
            if page_stats:
                print(f"   ‚úÖ {len(page_stats)} jours de statistiques de pages r√©cup√©r√©s")
            else:
                print("   ‚ö†Ô∏è  Donn√©es r√©cup√©r√©es mais aucune statistique valide")
                page_stats = []
        else:
            print("   ‚ùå Impossible de r√©cup√©rer les statistiques de pages")
            page_stats = []
        
        # 2. R√©cup√©ration des statistiques de followers
        print(f"\n2. R√©cup√©ration des statistiques de followers...")
        follower_tracker = LinkedInFollowerStatisticsTracker(
            self.access_token, org_id, self.days_history
        )
        raw_follower_stats = follower_tracker.get_daily_follower_statistics()
        
        if raw_follower_stats:
            follower_stats = follower_tracker.parse_daily_follower_statistics(raw_follower_stats)
            if follower_stats:
                print(f"   ‚úÖ {len(follower_stats)} jours de statistiques de followers r√©cup√©r√©s")
            else:
                print("   ‚ö†Ô∏è  Donn√©es r√©cup√©r√©es mais aucune statistique valide")
                follower_stats = []
        else:
            print("   ‚ùå Impossible de r√©cup√©rer les statistiques de followers")
            follower_stats = []
        
        # 3. R√©cup√©ration des statistiques de partages
        print(f"\n3. R√©cup√©ration des statistiques de partages...")
        share_tracker = LinkedInShareStatisticsTracker(
            self.access_token, org_id, self.days_history
        )
        raw_share_stats = share_tracker.get_daily_share_statistics()
        
        if raw_share_stats:
            share_stats = share_tracker.parse_daily_share_statistics(raw_share_stats)
            if share_stats:
                print(f"   ‚úÖ {len(share_stats)} jours de statistiques de partages r√©cup√©r√©s")
            else:
                print("   ‚ö†Ô∏è  Donn√©es r√©cup√©r√©es mais aucune statistique valide")
                share_stats = []
        else:
            print("   ‚ùå Impossible de r√©cup√©rer les statistiques de partages")
            share_stats = []
        
        # V√©rifier si des donn√©es sont disponibles
        if not page_stats and not follower_stats and not share_stats:
            print("\n‚ùå Aucune donn√©e disponible pour cette organisation")
            return False
        
        # R√©sum√© des donn√©es collect√©es
        print(f"\nüìà R√©sum√© des donn√©es collect√©es:")
        print(f"   - Pages: {len(page_stats)} jours")
        print(f"   - Followers: {len(follower_stats)} jours")
        print(f"   - Partages: {len(share_stats)} jours")
        
        # 4. Export vers Google Sheets
        print(f"\n4. Export vers Google Sheets...")
        
        # Chemin vers les credentials
        credentials_path = Path(__file__).resolve().parent / 'credentials' / 'service_account_credentials.json'
        
        # Pour Google Cloud Run, utiliser le chemin mont√©
        if os.getenv('K_SERVICE'):  # Variable d'environnement de Cloud Run
            credentials_path = Path('/workspace/credentials/service_account_credentials.json')
        
        # Pour les environnements o√π les credentials sont dans /tmp
        if not credentials_path.exists() and os.path.exists('/tmp/credentials/service_account_credentials.json'):
            credentials_path = Path('/tmp/credentials/service_account_credentials.json')
        
        if not credentials_path.exists():
            # Essayer de cr√©er les credentials depuis une variable d'environnement
            creds_json = os.getenv('GOOGLE_SERVICE_ACCOUNT_JSON')
            if creds_json:
                credentials_path.parent.mkdir(parents=True, exist_ok=True)
                with open(credentials_path, 'w') as f:
                    f.write(creds_json)
                print("   ‚úÖ Credentials cr√©√©s depuis la variable d'environnement")
            else:
                print(f"   ‚ùå Erreur: Fichier de credentials non trouv√©: {credentials_path}")
                return False
        else:
            print("   ‚úÖ Credentials trouv√©s")
        
        exporter = GoogleSheetsExporter(sheet_name, credentials_path, self.admin_email)
        success = exporter.add_combined_statistics(page_stats, follower_stats, share_stats, org_id)
        
        if success and exporter.spreadsheet:
            # Mettre √† jour le mapping avec l'ID du sheet
            self.update_sheet_mapping(org_id, exporter.spreadsheet.id)
            print(f"\n‚úÖ Export r√©ussi pour {org_name}!")
            print(f"üìä URL du Sheet: https://docs.google.com/spreadsheets/d/{exporter.spreadsheet.id}")
        else:
            print(f"\n‚ùå √âchec de l'export pour {org_name}")
        
        return success


def main():
    """Fonction principale"""
    print("="*60)
    print("LINKEDIN MULTI-ORGANISATION STATISTICS TRACKER")
    print("Version am√©lior√©e avec mise √† jour forc√©e pour automatisation")
    print("="*60)
    print(f"Date d'ex√©cution: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Cr√©er le tracker
    tracker = MultiOrganizationTracker()
    
    if not tracker.organizations:
        print("\n‚ùå Aucune organisation configur√©e!")
        print("\nPour configurer des organisations:")
        print("1. Lancez d'abord: python3 discover_organizations.py")
        print("2. Ou cr√©ez manuellement 'organizations_config.json' avec le format:")
        print(json.dumps([
            {"id": "123456", "name": "Entreprise A"},
            {"id": "789012", "name": "Entreprise B"}
        ], indent=2))
        sys.exit(1)
    
    print(f"\nüìã Organisations configur√©es: {len(tracker.organizations)}")
    for org in tracker.organizations:
        print(f"   - {org['name']} (ID: {org['id']})")
    
    print(f"\n‚öôÔ∏è  Configuration:")
    print(f"   - Jours d'historique: {tracker.days_history}")
    print(f"   - Email admin: {tracker.admin_email}")
    print(f"   - Mode: {'ü§ñ AUTOMATIS√â' if tracker.is_automated else 'üë§ MANUEL'}")
    
    # En mode automatis√©, pas de demande de confirmation
    if not tracker.is_automated and len(tracker.organizations) > 3:
        print(f"\n‚ö†Ô∏è  Attention: {len(tracker.organizations)} organisations √† traiter.")
        print("   Cela peut prendre du temps et consommer des quotas API.")
        print("   Des pauses de 30 secondes seront appliqu√©es entre chaque organisation.")
        response = input("   Continuer ? (o/N): ")
        if response.lower() != 'o':
            print("Annul√©.")
            sys.exit(0)
    
    print("\nüöÄ D√©marrage du traitement avec gestion optimis√©e des quotas...")
    
    # Lancer le traitement
    start_time = datetime.now()
    success = tracker.process_all_organizations()
    end_time = datetime.now()
    
    # Afficher le temps d'ex√©cution
    duration = end_time - start_time
    minutes = int(duration.total_seconds() // 60)
    seconds = int(duration.total_seconds() % 60)
    
    print(f"\n‚è±Ô∏è  Temps d'ex√©cution: {minutes}m {seconds}s")
    
    # Nettoyer les fichiers de progression temporaires
    for file in os.listdir('.'):
        if file.startswith('linkedin_stats_progress_') and file.endswith('.json'):
            os.remove(file)
            print(f"üßπ Fichier temporaire supprim√©: {file}")
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()