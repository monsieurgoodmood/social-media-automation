"""
Script adapt√© pour collecter les m√©triques lifetime des posts Facebook dans Cloud Functions
Fait partie du syst√®me d'automatisation Facebook sur Google Cloud
"""
import os
import sys
import requests
import pandas as pd
from datetime import datetime, timedelta
import logging
import time
import json

# Ajouter le r√©pertoire parent au path pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from scripts.fb_base_collector import FacebookBaseCollector
except ImportError:
    # Pour les tests locaux
    from fb_base_collector import FacebookBaseCollector

logger = logging.getLogger(__name__)

# Liste compl√®te des m√©triques lifetime des posts
METRICS = [
    "post_impressions", "post_reactions_like_total", "post_video_views", "post_clicks",
    "post_impressions_organic", "post_reactions_love_total", "post_reactions_wow_total",
    "post_video_complete_views_30s", "post_video_avg_time_watched", "post_video_views_sound_on",
    "post_reactions_haha_total", "post_reactions_sorry_total", "post_reactions_anger_total",
    "post_consumptions", "post_video_views_unique", "post_video_views_organic_unique",
    "post_video_view_time", "post_impressions_paid", "post_impressions_viral", "post_video_views_paid",
    "post_video_views_paid_unique", "post_video_views_by_distribution_type", "post_video_retention_graph",
    "post_reactions_by_type_total", "post_fan_reach", "post_video_views_organic", "post_clicks_by_type",
    "post_impressions_fan", "post_impressions_nonviral", "post_impressions_unique", "post_impressions_viral_unique",
    "post_impressions_organic_unique", "post_impressions_paid_unique", "post_activity_by_action_type",
    "post_activity_by_action_type_unique", "post_video_followers", "post_video_social_actions",
    "post_video_view_time_by_region_id", "post_impressions_nonviral", "post_impressions_nonviral_unique"
]

def adjust_column_types(df):
    """
    Ajuste les types des colonnes selon leur nature :
    - Dates en datetime
    - M√©triques num√©riques en int ou float selon le cas
    - URLs et textes en string
    """
    # Colonnes de type datetime
    datetime_columns = ["created_time"]
    
    # Colonnes qui doivent √™tre des entiers
    integer_columns = [
        "post_impressions", "post_impressions_organic", "post_impressions_paid",
        "post_impressions_viral", "post_impressions_fan", "post_impressions_nonviral",
        "post_impressions_unique", "post_impressions_organic_unique",
        "post_impressions_paid_unique", "post_impressions_viral_unique",
        "post_reactions_like_total", "post_reactions_love_total",
        "post_reactions_wow_total", "post_reactions_haha_total",
        "post_reactions_sorry_total", "post_reactions_anger_total",
        "post_reactions_by_type_total_like", "post_reactions_by_type_total_love",
        "post_clicks", "post_clicks_by_type_other clicks",
        "post_clicks_by_type_link clicks", "post_clicks_by_type_photo view",
        "post_video_views", "post_video_views_organic", "post_video_views_paid",
        "post_video_views_unique", "post_video_views_organic_unique",
        "post_video_views_paid_unique", "post_video_views_sound_on",
        "post_video_complete_views_30s", "post_video_followers",
        "post_video_social_actions", "post_consumptions",
        "post_impressions_nonviral_unique"
    ]
    
    # Colonnes qui doivent √™tre des nombres d√©cimaux
    float_columns = [
        "post_video_avg_time_watched",
        "post_video_view_time"
    ]
    
    # Colonnes qui doivent √™tre des cha√Ænes de caract√®res
    string_columns = [
        "post_id", "message", "media_url", "media_embedded",
        "post_video_views_by_distribution_type_page_owned",
        "post_video_views_by_distribution_type_shared"
    ]

    try:
        # Traitement des dates
        for col in datetime_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors="coerce")


        # Traitement des entiers
        for col in integer_columns:
            if col in df.columns:
                df[col] = df[col].fillna(0).astype(int)

        # Traitement des nombres d√©cimaux
        for col in float_columns:
            if col in df.columns:
                df[col] = df[col].fillna(0.0).astype(float)

        # Traitement des cha√Ænes de caract√®res
        for col in string_columns:
            if col in df.columns:
                df[col] = df[col].fillna('').astype(str)

        # Pour toutes les autres colonnes non explicitement d√©finies
        remaining_columns = [col for col in df.columns if col not in 
                           datetime_columns + integer_columns + 
                           float_columns + string_columns]
        
        for col in remaining_columns:
            if pd.api.types.is_numeric_dtype(df[col]):
                if all(df[col].dropna() % 1 == 0):
                    df[col] = df[col].fillna(0).astype(int)
                else:
                    df[col] = df[col].fillna(0).astype(float)
            else:
                df[col] = df[col].fillna('').astype(str)

        return df

    except Exception as e:
        logger.error(f"Erreur lors de l'ajustement des types de colonnes: {str(e)}")
        raise

def add_calculated_metrics(df):
    """
    Ajoute les m√©triques calcul√©es au DataFrame de mani√®re optimis√©e
    """
    # Calculer toutes les m√©triques en une fois pour √©viter la fragmentation
    calculated_metrics = pd.DataFrame(index=df.index)
    
    # Taux de clic - en d√©cimal pour le format pourcentage
    calculated_metrics['taux_de_clic'] = df.apply(
        lambda row: (row.get('post_clicks', 0) / row.get('post_impressions', 1)) 
        if row.get('post_impressions', 0) > 0 else 0, 
        axis=1
    )
    
    # (J'aime + J'adore + Wow + Haha + Triste + En col√®re + Clics) / Impressions
    calculated_metrics['taux_engagement_complet'] = df.apply(
        lambda row: (
            row.get('post_reactions_like_total', 0) +
            row.get('post_reactions_love_total', 0) +
            row.get('post_reactions_wow_total', 0) +
            row.get('post_reactions_haha_total', 0) +
            row.get('post_reactions_sorry_total', 0) +
            row.get('post_reactions_anger_total', 0) +
            row.get('post_clicks', 0)
        ) / row.get('post_impressions', 1)
        if row.get('post_impressions', 0) > 0 else 0,
        axis=1
    )
    
    # R√©actions positives
    calculated_metrics['reactions_positives'] = (
        df.get('post_reactions_like_total', 0) + 
        df.get('post_reactions_love_total', 0) + 
        df.get('post_reactions_wow_total', 0) + 
        df.get('post_reactions_haha_total', 0)
    )
    
    # R√©actions n√©gatives
    calculated_metrics['reactions_negatives'] = (
        df.get('post_reactions_sorry_total', 0) + 
        df.get('post_reactions_anger_total', 0)
    )
    
    # Total des r√©actions
    calculated_metrics['total_reactions'] = (
        df.get('post_reactions_like_total', 0) + 
        df.get('post_reactions_love_total', 0) + 
        df.get('post_reactions_wow_total', 0) + 
        df.get('post_reactions_haha_total', 0) + 
        df.get('post_reactions_sorry_total', 0) + 
        df.get('post_reactions_anger_total', 0)
    )
    
    # Concat√©ner toutes les m√©triques calcul√©es en une fois
    return pd.concat([df, calculated_metrics], axis=1)

def rename_columns(df):
    """
    Renomme les colonnes du DataFrame - VERSION CORRIG√âE pour coh√©rence
    """
    # D'abord ajouter les m√©triques calcul√©es
    df = add_calculated_metrics(df)
    
    column_mapping = {
        # Informations de base
        "created_time": "Date de publication",
        "post_id": "ID publication",
        "media_embedded": "URL m√©dia",
        "media_url": "Lien m√©dia",
        "message": "Message",
        
        # M√©triques d'impressions
        "post_impressions": "Affichages publication",
        "post_impressions_organic": "Affichages organiques",
        "post_impressions_paid": "Affichages sponsoris√©s",
        "post_impressions_viral": "Affichages viraux",
        "post_impressions_fan": "Affichages par fans",
        "post_impressions_nonviral": "Affichages non viraux",
        "post_impressions_unique": "Visiteurs de la publication",
        "post_impressions_organic_unique": "Visiteurs organiques",
        "post_impressions_paid_unique": "Visiteurs via pub",
        "post_impressions_viral_unique": "Visiteurs viraux",
        "post_impressions_nonviral_unique": "Visiteurs non viraux",
        
        # R√©actions
        "post_reactions_like_total": "Nbre de \"J'aime\"",
        "post_reactions_love_total": "Nbre de \"J'adore\"",
        "post_reactions_wow_total": "Nbre de \"Wow\"",
        "post_reactions_haha_total": "Nbre de \"Haha\"",
        "post_reactions_sorry_total": "Nbre de \"Triste\"",
        "post_reactions_anger_total": "Nbre de \"En col√®re\"",
        "post_reactions_by_type_total_like": "R√©actions J'aime",
        "post_reactions_by_type_total_love": "R√©actions J'adore",
        
        # Clics - üî• CORRECTION CRITIQUE
        "post_clicks": "Nbre de clics",  # ‚úÖ √âTAIT "Clics totaux"
        "post_clicks_by_type_other clicks": "Autres clics",
        "post_clicks_by_type_link clicks": "Clics sur liens",
        "post_clicks_by_type_photo view": "Clics sur photos",
        
        # M√©triques vid√©o
        "post_video_views": "Vues vid√©o",
        "post_video_views_organic": "Vues vid√©o organiques",
        "post_video_views_paid": "Vues vid√©o sponsoris√©es",
        "post_video_views_unique": "Visiteurs vid√©o uniques",
        "post_video_views_organic_unique": "Visiteurs vid√©o organiques",
        "post_video_views_paid_unique": "Visiteurs vid√©o sponsoris√©s",
        "post_video_views_sound_on": "Vues avec son",
        "post_video_complete_views_30s": "Vues compl√®tes (30s)",
        "post_video_avg_time_watched": "Temps moyen visionn√©",
        "post_video_view_time": "Dur√©e totale visionnage",
        "post_video_views_by_distribution_type_page_owned": "Vues sur la page",
        "post_video_views_by_distribution_type_shared": "Vues via partages",
        "post_video_followers": "Nouveaux abonn√©s vid√©o",
        "post_video_social_actions": "Interactions vid√©o",
        
        # Autres m√©triques
        "post_fan_reach": "Port√©e fans",
        "post_activity_by_action_type_share": "Partages",
        "post_activity_by_action_type_like": "J'aime sur activit√©",
        "post_activity_by_action_type_unique_share": "Partages uniques",
        "post_activity_by_action_type_unique_like": "J'aime uniques",
        "post_consumptions": "Interactions totales",
        
        # üî• CORRECTIONS CRITIQUES pour les commentaires:
        "post_activity_by_action_type_comment": "Nbre de commentaires",  # ‚úÖ COH√âRENT
        "post_activity_by_action_type_unique_comment": "Commentaires uniques",  # ‚ûï AJOUT√â
        
        # M√©triques calcul√©es
        "taux_de_clic": "Tx de clic (%)",
        "taux_engagement_complet": "Tx d'engagement (%)",
        "reactions_positives": "R√©actions positives",
        "reactions_negatives": "R√©actions n√©gatives",
        "total_reactions": "Total r√©actions"
    }
    
    return df.rename(columns=column_mapping)


def reorder_columns(df):
    """
    R√©organise les colonnes selon l'ordre sp√©cifi√©
    """
    main_columns = [
        "created_time", "post_id", "media_embedded", "media_url", "message",
        "post_impressions", "post_impressions_organic", "post_impressions_paid", "post_impressions_viral",
        "post_impressions_fan", "post_impressions_nonviral", "post_impressions_unique",
        "post_impressions_organic_unique", "post_impressions_paid_unique", "post_impressions_viral_unique",
        "post_reactions_like_total", "post_reactions_love_total", "post_reactions_wow_total",
        "post_reactions_haha_total", "post_reactions_sorry_total", "post_reactions_anger_total",
        "post_reactions_by_type_total_like", "post_reactions_by_type_total_love",
        "post_clicks", "post_clicks_by_type_other clicks", "post_clicks_by_type_link clicks",
        "post_clicks_by_type_photo view",
        "post_video_views", "post_video_views_organic", "post_video_views_paid", "post_video_views_unique",
        "post_video_views_organic_unique", "post_video_views_paid_unique", "post_video_views_sound_on",
        "post_video_complete_views_30s", "post_video_avg_time_watched",
        "post_video_views_by_distribution_type_page_owned", "post_video_views_by_distribution_type_shared",
        "post_fan_reach", "post_video_view_time",
        "post_activity_by_action_type_share", "post_activity_by_action_type_like",
        "post_activity_by_action_type_unique_share", "post_activity_by_action_type_unique_like",
        "post_video_followers", "post_video_social_actions", "post_consumptions",
        "post_impressions_nonviral_unique",
        # Ajouter les m√©triques calcul√©es √† la fin
        "taux_de_clic", "taux_engagement_complet", "reactions_positives", "reactions_negatives", "total_reactions"
    ]
    
    # Garder les colonnes dans l'ordre sp√©cifi√© si elles existent
    ordered_columns = [col for col in main_columns if col in df.columns]
    
    # Ajouter les colonnes restantes √† la fin
    other_columns = [col for col in df.columns if col not in main_columns]
    
    return df[ordered_columns + other_columns]



def retry_with_backoff(func, max_retries=3, initial_delay=2):
    """Wrapper pour r√©essayer une fonction avec un d√©lai exponentiel"""
    def wrapper(*args, **kwargs):
        delay = initial_delay
        last_exception = None
        
        for retry in range(max_retries):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                last_exception = e
                if "An unknown error has occurred" in str(e):
                    logger.warning(f"Tentative {retry + 1}/{max_retries} √©chou√©e. Nouvelle tentative dans {delay} secondes...")
                    time.sleep(delay)
                    delay *= 2  # Backoff exponentiel
                else:
                    raise  # Si ce n'est pas une erreur temporaire, on la propage
        
        raise last_exception
    
    return wrapper

def clean_dataframe(df):
    """
    Nettoie le DataFrame en supprimant les colonnes inutiles
    """
    # Liste des colonnes √† supprimer (commence par ces pr√©fixes)
    columns_to_remove_prefixes = [
        'post_video_view_time_by_region_id_',
        'post_video_retention_graph_',
        'post_video_view_time_by_age_bucket_and_gender_',
        'post_video_view_time_by_country_id_'
    ]
    
    # Identifier toutes les colonnes √† supprimer
    columns_to_drop = []
    for col in df.columns:
        for prefix in columns_to_remove_prefixes:
            if col.startswith(prefix):
                columns_to_drop.append(col)
                break
    
    # Supprimer les colonnes
    if columns_to_drop:
        logger.info(f"Suppression de {len(columns_to_drop)} colonnes de donn√©es r√©gionales/d√©mographiques d√©taill√©es")
        df = df.drop(columns=columns_to_drop, errors='ignore')
    
    return df

class FacebookPostsCollector:
    """
    Collecteur sp√©cialis√© pour les m√©triques lifetime des posts
    """
    def __init__(self, page_token, page_id):
        self.page_token = page_token
        self.page_id = page_id
        self.api_version = "v21.0"
        self.base_url = f"https://graph.facebook.com/{self.api_version}"

    def get_posts(self, since_date):
        """R√©cup√®re tous les posts depuis une date donn√©e"""
        url = f"{self.base_url}/{self.page_id}/posts"
        params = {
            "access_token": self.page_token,
            "fields": "id,created_time,message",
            "since": since_date,
            "limit": 100
        }
        
        all_posts = []
        try:
            while url:
                response = requests.get(url, params=params)
                response.raise_for_status()
                data = response.json()
                
                if "error" in data:
                    logger.error(f"Erreur API Facebook: {data['error']}")
                    raise ValueError(f"Erreur API Facebook: {data['error'].get('message')}")
                
                if "data" in data:
                    all_posts.extend(data["data"])
                    logger.info(f"R√©cup√©r√© {len(data['data'])} posts")
                
                url = data.get("paging", {}).get("next")
                if url:
                    params = {}
                    time.sleep(1)
            
            return all_posts
            
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des posts: {e}")
            raise

    @retry_with_backoff
    def get_metrics(self, post_id):
        """R√©cup√®re toutes les m√©triques pour un post avec retry en cas d'erreur"""
        metrics_data = {
            "post_id": post_id,
            "media_url": None,
            "media_embedded": None
        }

        try:
            # R√©cup√©rer les m√©triques
            response = requests.get(
                f"{self.base_url}/{post_id}/insights",
                params={
                    "access_token": self.page_token,
                    "metric": ",".join(METRICS),
                    "period": "lifetime"
                }
            ).json()

            if "error" in response:
                logger.error(f"Erreur API metrics: {response['error']}")
                raise ValueError(f"Erreur API metrics: {response['error'].get('message')}")

            if "data" in response:
                for metric in response["data"]:
                    if metric["values"]:
                        value = metric["values"][0]["value"]
                        metric_name = metric["name"]
                        
                        # Filtrer les m√©triques r√©gionales d√©taill√©es
                        if metric_name == "post_video_view_time_by_region_id":
                            # On ne traite pas cette m√©trique
                            continue
                        elif metric_name == "post_video_retention_graph":
                            # On ne traite pas cette m√©trique non plus
                            continue
                        elif isinstance(value, dict):
                            # Pour les autres m√©triques dict, on garde seulement les cl√©s principales
                            for k, v in value.items():
                                # Filtrer les cl√©s qui sont des IDs de r√©gion ou des donn√©es trop d√©taill√©es
                                if not k.isdigit() and not k.startswith('U.'):
                                    metrics_data[f"{metric_name}_{k}"] = v
                        else:
                            metrics_data[metric_name] = value

            # R√©cup√©rer les d√©tails du m√©dia
            time.sleep(0.5)  # Petit d√©lai entre les requ√™tes
            details = requests.get(
                f"{self.base_url}/{post_id}",
                params={
                    "access_token": self.page_token,
                    "fields": "attachments"
                }
            ).json()

            if "attachments" in details:
                attachments = details["attachments"].get("data", [])
                if attachments:
                    media = attachments[0].get("media", {})
                    if "image" in media:
                        url = media["image"]["src"]
                        metrics_data["media_url"] = url
                        metrics_data["media_embedded"] = url
                    elif "source" in media:
                        url = media["source"]
                        metrics_data["media_url"] = url
                        metrics_data["media_embedded"] = f"[VIDEO] {url}"

            return metrics_data

        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des m√©triques pour {post_id}: {e}")
            raise


def get_page_token(user_token, page_id):
    """
    R√©cup√®re le token sp√©cifique √† une page Facebook
    Version am√©lior√©e qui utilise d'abord les tokens sauvegard√©s
    """
    import json  # Import local pour s'assurer qu'il est disponible
    import os
    from datetime import datetime
    
    # D'abord essayer de r√©cup√©rer le token depuis la configuration sauvegard√©e
    try:
        # Essayer avec ConfigManager si disponible (Cloud Functions)
        try:
            from utils.config_manager import ConfigManager
            config_manager = ConfigManager()
            tokens_config = config_manager.load_config("page_tokens.json")
            
            if tokens_config and page_id in tokens_config.get("tokens", {}):
                saved_token = tokens_config["tokens"][page_id].get("access_token")
                if saved_token:
                    logger.info(f"Token trouv√© dans la configuration pour la page {page_id}")
                    return saved_token
        except ImportError:
            # En local, charger depuis le fichier
            if os.path.exists("configs/page_tokens.json"):
                with open("configs/page_tokens.json", 'r') as f:
                    tokens_config = json.load(f)
                    
                if page_id in tokens_config.get("tokens", {}):
                    saved_token = tokens_config["tokens"][page_id].get("access_token")
                    if saved_token:
                        logger.info(f"Token trouv√© dans le fichier local pour la page {page_id}")
                        return saved_token
    except Exception as e:
        logger.warning(f"Impossible de charger les tokens sauvegard√©s: {e}")
    
    # Si pas de token sauvegard√©, utiliser l'API Facebook
    logger.info(f"R√©cup√©ration du token via l'API pour la page {page_id}")
    url = f"https://graph.facebook.com/v21.0/me/accounts"
    params = {"access_token": user_token}
    
    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()
        
        if "error" in data:
            raise Exception(f"Erreur API: {data['error'].get('message')}")
        
        for page in data.get("data", []):
            if page["id"] == page_id:
                page_token = page["access_token"]
                
                # Sauvegarder ce token pour les prochaines utilisations
                try:
                    # Essayer de sauvegarder avec ConfigManager
                    try:
                        from utils.config_manager import ConfigManager
                        config_manager = ConfigManager()
                        tokens_config = config_manager.load_config("page_tokens.json") or {"tokens": {}}
                        
                        tokens_config["tokens"][page_id] = {
                            "page_name": page.get("name", "Unknown"),
                            "access_token": page_token
                        }
                        tokens_config["last_updated"] = datetime.now().isoformat()
                        
                        config_manager.save_config("page_tokens.json", tokens_config)
                        logger.info(f"Token sauvegard√© pour la page {page_id}")
                    except ImportError:
                        # En local
                        os.makedirs("configs", exist_ok=True)
                        tokens_file = "configs/page_tokens.json"
                        
                        if os.path.exists(tokens_file):
                            with open(tokens_file, 'r') as f:
                                tokens_config = json.load(f)
                        else:
                            tokens_config = {"tokens": {}}
                        
                        tokens_config["tokens"][page_id] = {
                            "page_name": page.get("name", "Unknown"),
                            "access_token": page_token
                        }
                        tokens_config["last_updated"] = datetime.now().isoformat()
                        
                        with open(tokens_file, 'w') as f:
                            json.dump(tokens_config, f, indent=2)
                        logger.info(f"Token sauvegard√© localement pour la page {page_id}")
                except Exception as e:
                    logger.warning(f"Impossible de sauvegarder le token: {e}")
                
                return page_token
        
        raise Exception(f"Page {page_id} non trouv√©e ou non autoris√©e")
        
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration du token de page: {e}")
        raise

# Modifier process_posts_lifetime pour utiliser clean_dataframe
def process_posts_lifetime(token, page_id, page_name):
    """
    Fonction principale pour traiter les m√©triques lifetime des posts Facebook
    Compatible avec Cloud Functions
    
    Args:
        token: Token utilisateur Facebook
        page_id: ID de la page Facebook
        page_name: Nom de la page Facebook
        
    Returns:
        spreadsheet_id: ID du Google Sheet cr√©√©/mis √† jour
    """
    try:
        logger.info(f"D√©but du traitement des posts lifetime pour {page_name} ({page_id})")
        
        # R√©cup√©rer le token de la page
        page_token = get_page_token(token, page_id)
        
        # Initialiser les collecteurs
        collector = FacebookPostsCollector(page_token, page_id)
        base_collector = FacebookBaseCollector(page_token)
        
        # Obtenir ou cr√©er le spreadsheet
        spreadsheet_id = base_collector.get_or_update_spreadsheet(
            page_name, page_id, "posts_lifetime"
        )
        
        # R√©cup√©rer les posts des 24 derniers mois
        since_date = (datetime.now() - timedelta(days=730)).strftime('%Y-%m-%d')
        posts = collector.get_posts(since_date)
        
        logger.info(f"Nombre de posts √† traiter: {len(posts)}")
        
        # Collecter les m√©triques pour chaque post
        all_metrics = []
        errors = 0
        
        for i, post in enumerate(posts, 1):
            try:
                if i % 10 == 0:
                    logger.info(f"Progression: {i}/{len(posts)} posts trait√©s")
                
                metrics = collector.get_metrics(post["id"])
                metrics.update({
                    "created_time": post.get("created_time"),
                    "message": post.get("message", "")
                })
                
                # V√âRIFICATION FINALE : S'assurer qu'il y a toujours un message
                if not metrics.get("message") or metrics["message"] == "" or metrics["message"].strip() == "":
                    # Construire un message par d√©faut avec les informations disponibles
                    post_date = post.get("created_time", "")
                    if post_date:
                        try:
                            # Formater la date pour qu'elle soit lisible
                            date_obj = pd.to_datetime(post_date)
                            formatted_date = date_obj.strftime("%d/%m/%Y √† %H:%M")
                            metrics["message"] = f"[Publication du {formatted_date}]"
                        except:
                            metrics["message"] = f"[Publication - ID: {post.get('id', 'inconnu')}]"
                    else:
                        # Utiliser l'ID du post comme dernier recours
                        metrics["message"] = f"[Publication - ID: {post.get('id', 'inconnu')}]"
                
                all_metrics.append(metrics)
                
                # Respecter les limites de l'API
                time.sleep(2)
                
                # Pause plus longue tous les 50 posts
                if i % 50 == 0:
                    logger.info("Pause de 30 secondes pour respecter les limites d'API...")
                    time.sleep(30)
                    
            except Exception as e:
                logger.error(f"Erreur post {post['id']}: {e}")
                errors += 1
                continue
        
        if errors > 0:
            logger.warning(f"{errors} posts n'ont pas pu √™tre trait√©s")
        
        if all_metrics:
            # Cr√©er et formater le DataFrame
            df = pd.DataFrame(all_metrics)
            
            # IMPORTANT: Nettoyer les colonnes inutiles AVANT tout autre traitement
            df = clean_dataframe(df)
            
            # Ajuster les types de colonnes
            df = adjust_column_types(df)
            
            # R√©organiser les colonnes
            df = reorder_columns(df)
            
            # Renommer les colonnes (inclut l'ajout des m√©triques calcul√©es)
            df = rename_columns(df)
            
            # Mettre √† jour le Google Sheet
            base_collector.update_sheet_data(spreadsheet_id, df)
            
            logger.info(f"‚úì Posts lifetime mis √† jour pour {page_name}: {len(df)} posts")
        else:
            logger.warning(f"Aucune m√©trique de posts collect√©e pour {page_name}")
        
        return spreadsheet_id
        
    except Exception as e:
        logger.error(f"Erreur dans process_posts_lifetime pour {page_name}: {e}")
        raise

# Pour les tests locaux uniquement
if __name__ == "__main__":
    # Ce bloc ne s'ex√©cute que lors des tests locaux
    import sys
    from dotenv import load_dotenv
    
    load_dotenv()
    
    # Configuration de logging pour les tests
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    if len(sys.argv) != 4:
        print("Usage: python fb_posts_lifetime.py <token> <page_id> <page_name>")
        sys.exit(1)
    
    test_token = sys.argv[1]
    test_page_id = sys.argv[2]
    test_page_name = sys.argv[3]
    
    try:
        spreadsheet_id = process_posts_lifetime(test_token, test_page_id, test_page_name)
        print(f"‚úì Test r√©ussi! Spreadsheet: https://docs.google.com/spreadsheets/d/{spreadsheet_id}")
    except Exception as e:
        print(f"‚úó Erreur lors du test: {e}")
        sys.exit(1)