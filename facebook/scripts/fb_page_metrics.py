"""
Script adapt√© pour collecter les m√©triques de page Facebook dans Cloud Functions
Fait partie du syst√®me d'automatisation Facebook sur Google Cloud
"""
import os
import sys
import requests
import pandas as pd
from datetime import datetime, timedelta
import logging
import time
import json

# Ajouter le r√©pertoire parent au path pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from scripts.fb_base_collector import FacebookBaseCollector
except ImportError:
    # Pour les tests locaux
    from fb_base_collector import FacebookBaseCollector

logger = logging.getLogger(__name__)

# Liste compl√®te des m√©triques de page Facebook
METRICS = [
    # Page Impressions
    "page_impressions", 
    "page_impressions_unique", 
    "page_impressions_nonviral", 
    "page_impressions_viral",
    
    # Page Posts Impressions
    "page_posts_impressions", 
    "page_posts_impressions_unique", 
    "page_posts_impressions_paid", 
    "page_posts_impressions_organic", 
    "page_posts_impressions_organic_unique",
    
    # Page Views
    "page_views_total",

    # Page Fans and Follows
    "page_fans",
    "page_fan_adds", 
    "page_fan_removes",
    "page_fan_adds_by_paid_non_paid_unique",
    "page_follows",
    "page_daily_follows", 
    "page_daily_unfollows",
    "page_daily_follows_unique",

    # Video Metrics
    "page_video_views", 
    "page_video_views_unique", 
    "page_video_views_paid", 
    "page_video_views_organic",  
    "page_video_repeat_views", 
    "page_video_view_time",
    "page_video_complete_views_30s", 
    "page_video_complete_views_30s_unique", 
    "page_video_complete_views_30s_paid", 
    "page_video_complete_views_30s_organic", 
    "page_video_complete_views_30s_autoplayed", 
    "page_video_complete_views_30s_repeat_views",

    # Engagement Metrics
    "page_post_engagements", 
    "page_total_actions", 
    "page_actions_post_reactions_like_total", 
    "page_actions_post_reactions_love_total", 
    "page_actions_post_reactions_wow_total", 
    "page_actions_post_reactions_haha_total", 
    "page_actions_post_reactions_sorry_total", 
    "page_actions_post_reactions_anger_total"
]

def fetch_page_metrics(access_token, metrics, page_id, start_date, end_date):
    """
    R√©cup√®re les m√©triques pour une page sur toute la p√©riode par tranches de 90 jours
    Compatible avec les limites de l'API Facebook
    """
    graph_api_url = f"https://graph.facebook.com/v21.0/{page_id}/insights"
    all_data = []
    current_date = start_date
    
    logger.info(f"R√©cup√©ration des donn√©es pour la page {page_id}...")
    
    while current_date < end_date:
        # Calculer la fin de la tranche (90 jours maximum)
        chunk_end = min(current_date + timedelta(days=89), end_date)
        
        logger.info(f"- P√©riode du {current_date.strftime('%Y-%m-%d')} au {chunk_end.strftime('%Y-%m-%d')}")
        
        params = {
            "metric": ",".join(metrics),
            "period": "day",
            "access_token": access_token,
            # Convertir les dates en timestamps Unix pour l'API Facebook
            "since": int(current_date.timestamp()),
            "until": int(chunk_end.timestamp())
        }
        
        try:
            response = requests.get(graph_api_url, params=params)
            response.raise_for_status()
            data = response.json()

            if "error" in data:
                logger.error(f"Erreur API pour la p√©riode: {data['error'].get('message', '')}")
                if data['error'].get('code') == 190:  # Token invalide
                    raise Exception("Token invalide")
                continue

            if "data" in data:
                for metric_data in data["data"]:
                    metric_name = metric_data["name"]
                    for value in metric_data["values"]:
                        date = value.get("end_time", "").split("T")[0]
                        if date:
                            # Ajuster la date pour correspondre au jour r√©el des m√©triques
                            adjusted_date = (datetime.strptime(date, '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')
                            
                            # Cr√©er ou mettre √† jour l'enregistrement
                            existing_record = next((item for item in all_data if item["date"] == adjusted_date), None)
                            
                            if existing_record is None:
                                existing_record = {"date": adjusted_date}
                                all_data.append(existing_record)
                            
                            # Traiter la valeur m√©trique
                            metric_value = value.get("value", 0)
                            if isinstance(metric_value, dict):
                                for key, sub_value in metric_value.items():
                                    existing_record[f"{metric_name}_{key}"] = sub_value
                            else:
                                existing_record[metric_name] = metric_value

            # Attendre un peu entre les requ√™tes pour √©viter les limites de l'API
            time.sleep(1)

        except requests.exceptions.RequestException as e:
            logger.error(f"Erreur r√©seau sur la page {page_id} ({page_id}): {str(e)}")
            logger.warning(f"SKIP de cette p√©riode pour la page {page_id}, on continue avec la prochaine p√©riode...")
            time.sleep(5)
            continue

        except Exception as e:
            logger.error(f"Erreur inattendue: {str(e)}")
            if "Token invalide" in str(e):
                raise
            continue
        
        # Passer √† la prochaine tranche
        current_date = chunk_end + timedelta(days=1)

    if not all_data:
        logger.warning(f"Aucune donn√©e r√©cup√©r√©e pour la page {page_id} sur toute la p√©riode demand√©e.")
        # Retourner un DataFrame vide au lieu de continuer
        return pd.DataFrame()
    else:
        logger.info(f"‚úì Donn√©es r√©cup√©r√©es pour la page {page_id}: {len(all_data)} jours")

    # Cr√©er le DataFrame et trier par date
    df = pd.DataFrame(all_data)
    if not df.empty and "date" in df.columns:
        df = df.sort_values(by="date").fillna(0)
        logger.info(f"‚úì {len(df)} jours de donn√©es r√©cup√©r√©s")
    
    return df

def add_calculated_metrics(df):
    """
    Ajoute les m√©triques calcul√©es au DataFrame
    """
    # Taux d'engagement (page) - en d√©cimal pour le format pourcentage
    df['taux_engagement_page'] = df.apply(
        lambda row: (row.get('page_post_engagements', 0) / row.get('page_impressions', 1)) 
        if row.get('page_impressions', 0) > 0 else 0, 
        axis=1
    )
    
    # Fr√©quence des Impressions
    df['frequence_impressions'] = df.apply(
        lambda row: row.get('page_impressions', 0) / row.get('page_impressions_unique', 1) 
        if row.get('page_impressions_unique', 0) > 0 else 0, 
        axis=1
    )
    
    # Actions totales sur la page (corrig√©e)
    df['actions_totales_calculees'] = df.apply(
        lambda row: (
            row.get('page_total_actions', 0) + 
            row.get('page_actions_post_reactions_like_total', 0) + 
            row.get('page_actions_post_reactions_love_total', 0) + 
            row.get('page_actions_post_reactions_wow_total', 0) + 
            row.get('page_actions_post_reactions_haha_total', 0) + 
            row.get('page_actions_post_reactions_sorry_total', 0) + 
            row.get('page_actions_post_reactions_anger_total', 0)
        ), 
        axis=1
    )
    
    # VTR % (page) - en d√©cimal pour le format pourcentage
    df['vtr_percentage_page'] = df.apply(
        lambda row: (row.get('page_video_complete_views_30s', 0) / row.get('page_impressions', 1)) 
        if row.get('page_impressions', 0) > 0 else 0, 
        axis=1
    )
    
    return df

def format_dataframe(df):
    """
    Formate le DataFrame en renommant les colonnes et en ajustant les types de donn√©es.
    Inclut maintenant les m√©triques calcul√©es avec des noms simplifi√©s.
    """
    # D'abord ajouter les m√©triques calcul√©es
    df = add_calculated_metrics(df)
    
    # Configuration des colonnes avec leurs types et noms simplifi√©s
    column_config = {
        # Colonnes temporelles - datetime
        "date": {
            "new_name": "Date",
            "type": "datetime"
        },
        
        # M√©triques de base de la page - entiers
        "page_impressions": {
            "new_name": "Affichages de la page",  # Simplifi√©
            "type": "int"
        },
        "page_impressions_unique": {
            "new_name": "Visiteurs de la page",  # Simplifi√©
            "type": "int"
        },
        "page_impressions_nonviral": {
            "new_name": "Affichages non viraux",  # Simplifi√©
            "type": "int"
        },
        "page_impressions_viral": {
            "new_name": "Affichages viraux",  # Simplifi√©
            "type": "int"
        },
        
        # M√©triques des posts - entiers
        "page_posts_impressions": {
            "new_name": "Affichages des publications",  # Simplifi√©
            "type": "int"
        },
        "page_posts_impressions_unique": {
            "new_name": "Visiteurs de la publication",  # Simplifi√©
            "type": "int"
        },
        "page_posts_impressions_paid": {
            "new_name": "Affichages publicitaires",  # Simplifi√©
            "type": "int"
        },
        "page_posts_impressions_organic": {
            "new_name": "Affichages organiques",  # Simplifi√©
            "type": "int"
        },
        "page_posts_impressions_organic_unique": {
            "new_name": "Visiteurs uniques organiques",  # Simplifi√©
            "type": "int"
        },
        
        # M√©triques des vues de page - entiers
        "page_views_total": {
            "new_name": "Vues totales de la page",  # Simplifi√©
            "type": "int"
        },
        
        # M√©triques des fans - entiers
        "page_fans": {
            "new_name": "Nombre de fans",  # Simplifi√©
            "type": "int"
        },
        "page_fan_adds": {
            "new_name": "Nouveaux fans",  # Simplifi√©
            "type": "int"
        },
        "page_fan_removes": {
            "new_name": "Fans perdus",  # Simplifi√©
            "type": "int"
        },
        
        # M√©triques des abonnements payants/non payants - entiers
        "page_fan_adds_by_paid_non_paid_unique_total": {
            "new_name": "Total nouveaux fans (payants + organiques)",  # Simplifi√©
            "type": "int"
        },
        "page_fan_adds_by_paid_non_paid_unique_paid": {
            "new_name": "Nouveaux fans via pub",  # Simplifi√©
            "type": "int"
        },
        "page_fan_adds_by_paid_non_paid_unique_unpaid": {
            "new_name": "Nouveaux fans organiques",  # Simplifi√©
            "type": "int"
        },
        
        # M√©triques des follows - entiers
        "page_follows": {
            "new_name": "Nombre d'abonn√©s",  # Simplifi√©
            "type": "int"
        },
        "page_daily_follows": {
            "new_name": "Nouveaux abonn√©s",  # Simplifi√©
            "type": "int"
        },
        "page_daily_unfollows": {
            "new_name": "D√©sabonnements",
            "type": "int"
        },
        "page_daily_follows_unique": {
            "new_name": "Abonn√©s uniques du jour",  # Simplifi√©
            "type": "int"
        },
        
        # M√©triques des vid√©os - entiers et floats
        "page_video_views": {
            "new_name": "Vues de vid√©os",  # Simplifi√©
            "type": "int"
        },
        "page_video_views_unique": {
            "new_name": "Vues uniques de vid√©os",  # Simplifi√©
            "type": "int"
        },
        "page_video_views_paid": {
            "new_name": "Vues vid√©os via pub",  # Simplifi√©
            "type": "int"
        },
        "page_video_views_organic": {
            "new_name": "Vues vid√©os organiques",  # Simplifi√©
            "type": "int"
        },
        "page_video_repeat_views": {
            "new_name": "Relectures vid√©os",  # Simplifi√©
            "type": "int"
        },
        "page_video_view_time": {
            "new_name": "Temps de visionnage (sec)",  # Simplifi√©
            "type": "float"
        },
        
        # M√©triques des vues compl√®tes - entiers
        "page_video_complete_views_30s": {
            "new_name": "Vues compl√®tes (30s)",  # Simplifi√© selon votre demande
            "type": "int"
        },
        "page_video_complete_views_30s_unique": {
            "new_name": "Vues compl√®tes uniques (30s)",  # Simplifi√©
            "type": "int"
        },
        "page_video_complete_views_30s_paid": {
            "new_name": "Vues compl√®tes via pub (30s)",  # Simplifi√©
            "type": "int"
        },
        "page_video_complete_views_30s_organic": {
            "new_name": "Vues compl√®tes organiques (30s)",  # Simplifi√©
            "type": "int"
        },
        "page_video_complete_views_30s_autoplayed": {
            "new_name": "Vues compl√®tes auto (30s)",  # Simplifi√©
            "type": "int"
        },
        "page_video_complete_views_30s_repeat_views": {
            "new_name": "Relectures compl√®tes (30s)",  # Simplifi√©
            "type": "int"
        },
        
        # M√©triques d'engagement - entiers
        "page_post_engagements": {
            "new_name": "Interactions sur publications",  # Simplifi√©
            "type": "int"
        },
        "page_total_actions": {
            "new_name": "Actions totales",  # Simplifi√©
            "type": "int"
        },
        "page_actions_post_reactions_like_total": {
            "new_name": "Nbre de \"J'aime\"",  # Simplifi√© selon votre demande
            "type": "int"
        },
        "page_actions_post_reactions_love_total": {
            "new_name": "Nbre de \"J'adore\"",  # Simplifi√© selon votre demande
            "type": "int"
        },
        "page_actions_post_reactions_wow_total": {
            "new_name": "Nbre de \"Wow\"",  # Simplifi√© selon votre demande
            "type": "int"
        },
        "page_actions_post_reactions_haha_total": {
            "new_name": "Nbre de \"Haha\"",  # Simplifi√© selon votre demande
            "type": "int"
        },
        "page_actions_post_reactions_sorry_total": {
            "new_name": "Nbre de \"Triste\"",  # Simplifi√© selon votre demande
            "type": "int"
        },
        "page_actions_post_reactions_anger_total": {
            "new_name": "Nbre de \"En col√®re\"",  # Simplifi√© selon votre demande
            "type": "int"
        },
        
        # M√©triques calcul√©es - floats
        "taux_engagement_page": {
            "new_name": "Tx d'engagement (%)",  # Simplifi√© avec Tx
            "type": "float"
        },
        "frequence_impressions": {
            "new_name": "Fr√©quence impressions",  # D√©j√† court
            "type": "float"
        },
        "actions_totales_calculees": {
            "new_name": "Actions calcul√©es",  # Simplifi√©
            "type": "int"
        },
        "vtr_percentage_page": {
            "new_name": "VTR %",  # Simplifi√©
            "type": "float"
        }
    }

    try:
        # Convertir la colonne date en datetime
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"])

        # Appliquer les types et renommages pour chaque colonne
        for col_name, config in column_config.items():
            if col_name in df.columns:
                if config["type"] == "int":
                    df[col_name] = pd.to_numeric(df[col_name], errors='coerce').fillna(0).astype(int)
                elif config["type"] == "float":
                    df[col_name] = pd.to_numeric(df[col_name], errors='coerce').fillna(0.0).astype(float)
                elif config["type"] == "datetime":
                    df[col_name] = pd.to_datetime(df[col_name])

        # Renommer les colonnes
        rename_mapping = {k: v["new_name"] for k, v in column_config.items() if k in df.columns}
        df = df.rename(columns=rename_mapping)

        return df

    except Exception as e:
        logger.error(f"Erreur lors du formatage du DataFrame: {str(e)}")
        raise
    

def get_page_token(user_token, page_id):
    """
    R√©cup√®re le token sp√©cifique √† une page Facebook
    Version am√©lior√©e qui utilise d'abord les tokens sauvegard√©s
    """
    import json  # Import local pour s'assurer qu'il est disponible
    import os
    from datetime import datetime
    
    # D'abord essayer de r√©cup√©rer le token depuis la configuration sauvegard√©e
    try:
        # Essayer avec ConfigManager si disponible (Cloud Functions)
        try:
            from utils.config_manager import ConfigManager
            config_manager = ConfigManager()
            tokens_config = config_manager.load_config("page_tokens.json")
            
            if tokens_config and page_id in tokens_config.get("tokens", {}):
                saved_token = tokens_config["tokens"][page_id].get("access_token")
                if saved_token:
                    logger.info(f"Token trouv√© dans la configuration pour la page {page_id}")
                    return saved_token
        except ImportError:
            # En local, charger depuis le fichier
            if os.path.exists("configs/page_tokens.json"):
                with open("configs/page_tokens.json", 'r') as f:
                    tokens_config = json.load(f)
                    
                if page_id in tokens_config.get("tokens", {}):
                    saved_token = tokens_config["tokens"][page_id].get("access_token")
                    if saved_token:
                        logger.info(f"Token trouv√© dans le fichier local pour la page {page_id}")
                        return saved_token
    except Exception as e:
        logger.warning(f"Impossible de charger les tokens sauvegard√©s: {e}")
    
    # Si pas de token sauvegard√©, utiliser l'API Facebook
    logger.info(f"R√©cup√©ration du token via l'API pour la page {page_id}")
    url = f"https://graph.facebook.com/v21.0/me/accounts"
    params = {"access_token": user_token}
    
    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()
        
        if "error" in data:
            raise Exception(f"Erreur API: {data['error'].get('message')}")
        
        for page in data.get("data", []):
            if page["id"] == page_id:
                page_token = page["access_token"]
                
                # Sauvegarder ce token pour les prochaines utilisations
                try:
                    # Essayer de sauvegarder avec ConfigManager
                    try:
                        from utils.config_manager import ConfigManager
                        config_manager = ConfigManager()
                        tokens_config = config_manager.load_config("page_tokens.json") or {"tokens": {}}
                        
                        tokens_config["tokens"][page_id] = {
                            "page_name": page.get("name", "Unknown"),
                            "access_token": page_token
                        }
                        tokens_config["last_updated"] = datetime.now().isoformat()
                        
                        config_manager.save_config("page_tokens.json", tokens_config)
                        logger.info(f"Token sauvegard√© pour la page {page_id}")
                    except ImportError:
                        # En local
                        os.makedirs("configs", exist_ok=True)
                        tokens_file = "configs/page_tokens.json"
                        
                        if os.path.exists(tokens_file):
                            with open(tokens_file, 'r') as f:
                                tokens_config = json.load(f)
                        else:
                            tokens_config = {"tokens": {}}
                        
                        tokens_config["tokens"][page_id] = {
                            "page_name": page.get("name", "Unknown"),
                            "access_token": page_token
                        }
                        tokens_config["last_updated"] = datetime.now().isoformat()
                        
                        with open(tokens_file, 'w') as f:
                            json.dump(tokens_config, f, indent=2)
                        logger.info(f"Token sauvegard√© localement pour la page {page_id}")
                except Exception as e:
                    logger.warning(f"Impossible de sauvegarder le token: {e}")
                
                return page_token
        
        raise Exception(f"Page {page_id} non trouv√©e ou non autoris√©e")
        
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration du token de page: {e}")
        raise
    
    
def process_page_metrics(token, page_id, page_name):
    """
    Fonction principale pour traiter les m√©triques d'une page Facebook
    Compatible avec Cloud Functions
    
    Args:
        token: Token utilisateur Facebook
        page_id: ID de la page Facebook
        page_name: Nom de la page Facebook
        
    Returns:
        spreadsheet_id: ID du Google Sheet cr√©√©/mis √† jour (ou None si erreur bloquante)
    """
    try:
        logger.info(f"üöÄ D√©but du traitement des m√©triques pour {page_name} ({page_id})")
        
        # R√©cup√©rer le token de la page
        page_access_token = get_page_token(token, page_id)
        
        # Initialiser le collecteur de base
        collector = FacebookBaseCollector(page_access_token)
        
        # Obtenir ou cr√©er le spreadsheet
        spreadsheet_id = collector.get_or_update_spreadsheet(page_name, page_id, "page_metrics")
        
        # D√©finir la p√©riode de collecte
        end_date = datetime.now()

        # Dates existantes
        existing_dates = collector.get_existing_dates(spreadsheet_id)

        # Si peu de dates ‚Üí initial load
        if len(existing_dates) < 30:
            logger.info(f"Mode initial load pour {page_name} ‚Üí r√©cup√©ration de 2 ans de donn√©es")
            start_date = end_date.replace(year=end_date.year - 2)
            update_mode = "initial"

        else:
            # Daily update : on r√©cup√®re √† partir de la derni√®re date existante
            logger.info(f"Mode daily update pour {page_name}")
            
            # Normaliser et parser les dates existantes
            parsed_dates = []
            for d in existing_dates:
                try:
                    parsed_dates.append(pd.to_datetime(d).date())
                except:
                    continue
            
            if parsed_dates:
                # Trouver la date la plus r√©cente
                latest_date = max(parsed_dates)
                # Commencer le lendemain de la derni√®re date
                start_date = datetime.combine(latest_date + timedelta(days=1), datetime.min.time())
                logger.info(f"‚Üí Derni√®re date existante: {latest_date}, on r√©cup√®re depuis {start_date.date()}")
            else:
                # Si on ne peut pas parser les dates, on fait une update des 7 derniers jours
                start_date = end_date - timedelta(days=7)
                logger.info("‚Üí Impossible de parser les dates existantes, on update les 7 derniers jours")
            
            update_mode = "daily"
        
        try:
            # R√©cup√©rer les m√©triques
            raw_data = fetch_page_metrics(page_access_token, METRICS, page_id, start_date, end_date)

            if raw_data.empty:
                logger.info(f"‚úì Aucune nouvelle donn√©e √† r√©cup√©rer pour {page_name} (donn√©es d√©j√† √† jour)")
                return spreadsheet_id
                
            if update_mode == "daily":
                # En mode daily ‚Üí ne garder que les dates qui n'existent pas encore
                normalized_existing_dates = set()
                for d in existing_dates:
                    try:
                        parsed = pd.to_datetime(d).strftime("%Y-%m-%d")
                        normalized_existing_dates.add(parsed)
                    except:
                        continue
                
                # Filtrer pour ne garder que les nouvelles dates
                raw_data_dates = set(raw_data['date'].values)
                dates_to_keep = raw_data_dates - normalized_existing_dates
                
                if dates_to_keep:
                    raw_data = raw_data[raw_data['date'].isin(dates_to_keep)]
                    logger.info(f"‚Üí {len(raw_data)} nouvelles dates √† ins√©rer")
                else:
                    logger.info(f"‚úì Aucune nouvelle date √† ins√©rer pour {page_name} (toutes les dates sont √† jour)")
                    return spreadsheet_id
            else:
                # En mode initial, on garde toutes les donn√©es r√©cup√©r√©es
                logger.info(f"‚Üí Mode initial: {len(raw_data)} dates √† ins√©rer")

            # Formater les donn√©es (inclut maintenant les m√©triques calcul√©es)
            formatted_data = format_dataframe(raw_data)

            # Mettre √† jour le Google Sheet
            collector.update_sheet_data(spreadsheet_id, formatted_data)
            
            logger.info(f"‚úì M√©triques mises √† jour pour {page_name}: {len(formatted_data)} lignes")

        except Exception as e:
            logger.error(f"‚ùå Erreur bloquante pour la page {page_name} ({page_id}): {e} ‚Üí on passe √† la page suivante.")
            # On ne l√®ve pas l'erreur ‚Üí on passe √† la suivante
            return None
        
        return spreadsheet_id
        
    except Exception as e:
        logger.error(f"‚ùå Erreur g√©n√©rale dans process_page_metrics pour {page_name}: {e} ‚Üí on passe √† la page suivante.")
        return None


# Pour les tests locaux uniquement
if __name__ == "__main__":
    # Ce bloc ne s'ex√©cute que lors des tests locaux
    import sys
    from dotenv import load_dotenv
    
    load_dotenv()
    
    # Configuration de logging pour les tests
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    if len(sys.argv) != 4:
        print("Usage: python fb_page_metrics.py <token> <page_id> <page_name>")
        sys.exit(1)
    
    test_token = sys.argv[1]
    test_page_id = sys.argv[2]
    test_page_name = sys.argv[3]
    
    try:
        spreadsheet_id = process_page_metrics(test_token, test_page_id, test_page_name)
        print(f"‚úì Test r√©ussi! Spreadsheet: https://docs.google.com/spreadsheets/d/{spreadsheet_id}")
    except Exception as e:
        print(f"‚úó Erreur lors du test: {e}")
        sys.exit(1)