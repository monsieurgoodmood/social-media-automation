"""
Script adapt√© pour collecter les m√©triques de page Facebook dans Cloud Functions
Fait partie du syst√®me d'automatisation Facebook sur Google Cloud
VERSION CORRIG√âE - GESTION CORRECTE DE L'HISTORIQUE
"""
import os
import sys
import requests
import pandas as pd
from datetime import datetime, timedelta
import logging
import time
import json

# Ajouter le r√©pertoire parent au path pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from scripts.fb_base_collector import FacebookBaseCollector
except ImportError:
    # Pour les tests locaux
    from fb_base_collector import FacebookBaseCollector

logger = logging.getLogger(__name__)

# Liste compl√®te des m√©triques de page Facebook
METRICS = [
    # Page Impressions
    "page_impressions", 
    "page_impressions_unique", 
    "page_impressions_nonviral", 
    "page_impressions_viral",
    
    # Page Posts Impressions
    "page_posts_impressions", 
    "page_posts_impressions_unique", 
    "page_posts_impressions_paid", 
    "page_posts_impressions_organic", 
    "page_posts_impressions_organic_unique",
    
    # Page Views
    "page_views_total",

    # Page Fans and Follows
    "page_fans",
    "page_fan_adds", 
    "page_fan_removes",
    "page_fan_adds_by_paid_non_paid_unique",
    "page_follows",
    "page_daily_follows", 
    "page_daily_unfollows",
    "page_daily_follows_unique",

    # Video Metrics
    "page_video_views", 
    "page_video_views_unique", 
    "page_video_views_paid", 
    "page_video_views_organic",  
    "page_video_repeat_views", 
    "page_video_view_time",
    "page_video_complete_views_30s", 
    "page_video_complete_views_30s_unique", 
    "page_video_complete_views_30s_paid", 
    "page_video_complete_views_30s_organic", 
    "page_video_complete_views_30s_autoplayed", 
    "page_video_complete_views_30s_repeat_views",

    # Engagement Metrics
    "page_post_engagements", 
    "page_total_actions", 
    "page_actions_post_reactions_like_total", 
    "page_actions_post_reactions_love_total", 
    "page_actions_post_reactions_wow_total", 
    "page_actions_post_reactions_haha_total", 
    "page_actions_post_reactions_sorry_total", 
    "page_actions_post_reactions_anger_total"
]

def fetch_page_metrics(access_token, metrics, page_id, start_date, end_date):
    """
    R√©cup√®re les m√©triques pour une page sur toute la p√©riode par tranches de 90 jours
    Compatible avec les limites de l'API Facebook
    """
    graph_api_url = f"https://graph.facebook.com/v21.0/{page_id}/insights"
    all_data = []
    current_date = start_date
    
    logger.info(f"R√©cup√©ration des donn√©es pour la page {page_id} du {start_date.strftime('%Y-%m-%d')} au {end_date.strftime('%Y-%m-%d')}")
    
    while current_date < end_date:
        # Calculer la fin de la tranche (90 jours maximum)
        chunk_end = min(current_date + timedelta(days=89), end_date)
        
        logger.info(f"- P√©riode du {current_date.strftime('%Y-%m-%d')} au {chunk_end.strftime('%Y-%m-%d')}")
        
        params = {
            "metric": ",".join(metrics),
            "period": "day",
            "access_token": access_token,
            # Convertir les dates en timestamps Unix pour l'API Facebook
            "since": int(current_date.timestamp()),
            "until": int(chunk_end.timestamp())
        }
        
        try:
            response = requests.get(graph_api_url, params=params)
            response.raise_for_status()
            data = response.json()

            if "error" in data:
                logger.error(f"Erreur API pour la p√©riode: {data['error'].get('message', '')}")
                if data['error'].get('code') == 190:  # Token invalide
                    raise Exception("Token invalide")
                current_date = chunk_end + timedelta(days=1)
                continue

            if "data" in data:
                for metric_data in data["data"]:
                    metric_name = metric_data["name"]
                    for value in metric_data["values"]:
                        date = value.get("end_time", "").split("T")[0]
                        if date:
                            # Ajuster la date pour correspondre au jour r√©el des m√©triques
                            adjusted_date = (datetime.strptime(date, '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')
                            
                            # Cr√©er ou mettre √† jour l'enregistrement
                            existing_record = next((item for item in all_data if item["date"] == adjusted_date), None)
                            
                            if existing_record is None:
                                existing_record = {"date": adjusted_date}
                                all_data.append(existing_record)
                            
                            # Traiter la valeur m√©trique
                            metric_value = value.get("value", 0)
                            if isinstance(metric_value, dict):
                                for key, sub_value in metric_value.items():
                                    existing_record[f"{metric_name}_{key}"] = sub_value
                            else:
                                existing_record[metric_name] = metric_value

            # Attendre un peu entre les requ√™tes pour √©viter les limites de l'API
            time.sleep(1)

        except requests.exceptions.RequestException as e:
            logger.error(f"Erreur r√©seau sur la page {page_id}: {str(e)}")
            logger.warning(f"SKIP de cette p√©riode, on continue avec la prochaine...")
            time.sleep(5)

        except Exception as e:
            logger.error(f"Erreur inattendue: {str(e)}")
            if "Token invalide" in str(e):
                raise
        
        # Passer √† la prochaine tranche
        current_date = chunk_end + timedelta(days=1)

    if not all_data:
        logger.warning(f"Aucune donn√©e r√©cup√©r√©e pour la page {page_id} sur la p√©riode demand√©e.")
        return pd.DataFrame()
    else:
        logger.info(f"‚úì Donn√©es r√©cup√©r√©es pour la page {page_id}: {len(all_data)} jours")

    # Cr√©er le DataFrame et trier par date
    df = pd.DataFrame(all_data)
    if not df.empty and "date" in df.columns:
        df = df.sort_values(by="date").fillna(0)
        logger.info(f"‚úì {len(df)} jours de donn√©es r√©cup√©r√©s")
    
    return df

def add_calculated_metrics(df):
    """
    Ajoute les m√©triques calcul√©es au DataFrame
    """
    # Taux d'engagement (page) - en d√©cimal pour le format pourcentage
    df['taux_engagement_page'] = df.apply(
        lambda row: (row.get('page_post_engagements', 0) / row.get('page_impressions', 1)) 
        if row.get('page_impressions', 0) > 0 else 0, 
        axis=1
    )
    
    # Fr√©quence des Impressions
    df['frequence_impressions'] = df.apply(
        lambda row: row.get('page_impressions', 0) / row.get('page_impressions_unique', 1) 
        if row.get('page_impressions_unique', 0) > 0 else 0, 
        axis=1
    )
    
    # Actions totales sur la page (corrig√©e)
    df['actions_totales_calculees'] = df.apply(
        lambda row: (
            row.get('page_total_actions', 0) + 
            row.get('page_actions_post_reactions_like_total', 0) + 
            row.get('page_actions_post_reactions_love_total', 0) + 
            row.get('page_actions_post_reactions_wow_total', 0) + 
            row.get('page_actions_post_reactions_haha_total', 0) + 
            row.get('page_actions_post_reactions_sorry_total', 0) + 
            row.get('page_actions_post_reactions_anger_total', 0)
        ), 
        axis=1
    )
    
    # VTR % (page) - en d√©cimal pour le format pourcentage
    df['vtr_percentage_page'] = df.apply(
        lambda row: (row.get('page_video_complete_views_30s', 0) / row.get('page_impressions', 1)) 
        if row.get('page_impressions', 0) > 0 else 0, 
        axis=1
    )
    
    return df

def get_corrected_column_mapping():
    """
    Retourne le mapping corrig√© pour les noms de colonnes
    """
    return {
        # Colonnes temporelles
        "date": "Date",
        
        # M√©triques de base de la page
        "page_impressions": "Affichages de la page",
        "page_impressions_unique": "Visiteurs de la page",
        "page_impressions_nonviral": "Affichages non viraux",
        "page_impressions_viral": "Affichages viraux",
        
        # M√©triques des posts
        "page_posts_impressions": "Affichages des publications",
        "page_posts_impressions_unique": "Visiteurs de la publication",
        "page_posts_impressions_paid": "Affichages publicitaires",
        "page_posts_impressions_organic": "Affichages organiques",
        "page_posts_impressions_organic_unique": "Visiteurs uniques organiques",
        
        # M√©triques des vues de page
        "page_views_total": "Vues totales de la page",
        
        # M√©triques des fans - CORRIG√â
        "page_fans": "Nbre de fans",
        "page_fan_adds": "Nouveaux fans",
        "page_fan_removes": "Fans perdus",
        
        # M√©triques des abonnements payants/non payants
        "page_fan_adds_by_paid_non_paid_unique_total": "Total nouveaux fans (payants + organiques)",
        "page_fan_adds_by_paid_non_paid_unique_paid": "Nouveaux fans via pub",
        "page_fan_adds_by_paid_non_paid_unique_unpaid": "Nouveaux fans organiques",
        
        # M√©triques des follows - CORRIG√â
        "page_follows": "Nbre d'abonn√©s",
        "page_daily_follows": "Nouveaux abonn√©s",
        "page_daily_unfollows": "D√©sabonnements",
        "page_daily_follows_unique": "Abonn√©s uniques du jour",
        
        # M√©triques des vid√©os
        "page_video_views": "Vues de vid√©os",
        "page_video_views_unique": "Vues uniques de vid√©os",
        "page_video_views_paid": "Vues vid√©os via pub",
        "page_video_views_organic": "Vues vid√©os organiques",
        "page_video_repeat_views": "Relectures vid√©os",
        "page_video_view_time": "Temps de visionnage (sec)",
        
        # M√©triques des vues compl√®tes
        "page_video_complete_views_30s": "Vues compl√®tes (30s)",
        "page_video_complete_views_30s_unique": "Vues compl√®tes uniques (30s)",
        "page_video_complete_views_30s_paid": "Vues compl√®tes via pub (30s)",
        "page_video_complete_views_30s_organic": "Vues compl√®tes organiques (30s)",
        "page_video_complete_views_30s_autoplayed": "Vues compl√®tes auto (30s)",
        "page_video_complete_views_30s_repeat_views": "Relectures compl√®tes (30s)",
        
        # M√©triques d'engagement
        "page_post_engagements": "Interactions sur publications",
        "page_total_actions": "Actions totales",
        "page_actions_post_reactions_like_total": "Nbre de \"J'aime\"",
        "page_actions_post_reactions_love_total": "Nbre de \"J'adore\"",
        "page_actions_post_reactions_wow_total": "Nbre de \"Wow\"",
        "page_actions_post_reactions_haha_total": "Nbre de \"Haha\"",
        "page_actions_post_reactions_sorry_total": "Nbre de \"Triste\"",
        "page_actions_post_reactions_anger_total": "Nbre de \"En col√®re\"",
        
        # M√©triques calcul√©es - CORRIG√â
        "taux_engagement_page": "Tx d'engagement (%)",
        "frequence_impressions": "Fr√©quence des affichages",
        "actions_totales_calculees": "Actions calcul√©es",
        "vtr_percentage_page": "VTR %"
    }

def format_dataframe(df):
    """
    Formate le DataFrame avec les noms de colonnes CORRIG√âS et FORC√âS
    """
    # D'abord ajouter les m√©triques calcul√©es
    df = add_calculated_metrics(df)
    
    # Obtenir le mapping corrig√©
    column_mapping = get_corrected_column_mapping()
    
    try:
        # Convertir la colonne date en datetime
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"])

        # Appliquer les types de donn√©es pour chaque colonne
        for col_name in df.columns:
            if col_name == "date":
                df[col_name] = pd.to_datetime(df[col_name])
            elif col_name in ["taux_engagement_page", "frequence_impressions", "vtr_percentage_page", "page_video_view_time"]:
                df[col_name] = pd.to_numeric(df[col_name], errors='coerce').fillna(0.0).astype(float)
            else:
                # Toutes les autres colonnes sont des entiers
                df[col_name] = pd.to_numeric(df[col_name], errors='coerce').fillna(0).astype(int)

        # FORCER le renommage des colonnes - m√™me si elles existent d√©j√† avec d'autres noms
        df = df.rename(columns=column_mapping)
        
        # Log pour v√©rifier les colonnes finales
        logger.info(f"‚úÖ Colonnes apr√®s renommage: {list(df.columns)}")
        
        return df

    except Exception as e:
        logger.error(f"Erreur lors du formatage du DataFrame: {str(e)}")
        raise

def get_expected_columns():
    """
    Retourne la liste des colonnes attendues CORRIG√âES
    """
    return [
        "Date",
        "Affichages de la page",
        "Visiteurs de la page", 
        "Affichages non viraux",
        "Affichages viraux",
        "Affichages des publications",
        "Visiteurs de la publication",
        "Affichages publicitaires",
        "Affichages organiques",
        "Visiteurs uniques organiques",
        "Vues totales de la page",
        "Nbre de fans",
        "Nouveaux fans",
        "Fans perdus",
        "Total nouveaux fans (payants + organiques)",
        "Nouveaux fans via pub",
        "Nouveaux fans organiques",
        "Nbre d'abonn√©s",
        "Nouveaux abonn√©s",
        "D√©sabonnements",
        "Abonn√©s uniques du jour",
        "Vues de vid√©os",
        "Vues uniques de vid√©os",
        "Vues vid√©os via pub",
        "Vues vid√©os organiques",
        "Relectures vid√©os",
        "Temps de visionnage (sec)",
        "Vues compl√®tes (30s)",
        "Vues compl√®tes uniques (30s)",
        "Vues compl√®tes via pub (30s)",
        "Vues compl√®tes organiques (30s)",
        "Vues compl√®tes auto (30s)",
        "Relectures compl√®tes (30s)",
        "Interactions sur publications",
        "Actions totales",
        "Nbre de \"J'aime\"",
        "Nbre de \"J'adore\"",
        "Nbre de \"Wow\"",
        "Nbre de \"Haha\"",
        "Nbre de \"Triste\"",
        "Nbre de \"En col√®re\"",
        "Tx d'engagement (%)",
        "Fr√©quence des affichages",
        "Actions calcul√©es",
        "VTR %"
    ]

def force_correct_column_names(df):
    """
    Force la correction des noms de colonnes M√äME SI elles ont d√©j√† des noms incorrects
    """
    correction_mapping = {
        "Nombre de fans": "Nbre de fans",
        "Nombre d'abonn√©s": "Nbre d'abonn√©s", 
        "Fr√©quence des impressions": "Fr√©quence des affichages",
        "Fr√©quence impressions": "Fr√©quence des affichages",
        "Clics totaux": "Nbre de clics",
        "post_activity_by_action_type_comment": "Nbre de commentaires"
    }
    
    df = df.rename(columns=correction_mapping)
    logger.info(f"üîß Correction forc√©e des noms de colonnes appliqu√©e")
    
    return df

def validate_dataframe_columns(df, expected_columns=None):
    """
    Valide que le DataFrame a les bonnes colonnes avant l'envoi vers Google Sheets
    """
    if expected_columns is None:
        expected_columns = get_expected_columns()
    
    current_columns = list(df.columns)
    
    # FORCER la correction des noms AVANT validation
    df = force_correct_column_names(df)
    current_columns = list(df.columns)
    
    # V√©rifier les colonnes manquantes
    missing_columns = set(expected_columns) - set(current_columns)
    
    # V√©rifier les colonnes suppl√©mentaires 
    extra_columns = set(current_columns) - set(expected_columns)
    
    # Cr√©er une copie du DataFrame pour correction
    corrected_df = df.copy()
    
    errors = []
    warnings = []
    
    # Ajouter les colonnes manquantes avec des valeurs par d√©faut
    if missing_columns:
        for col in missing_columns:
            if "Date" in col:
                corrected_df[col] = pd.NaT
            elif any(keyword in col.lower() for keyword in ["tx", "vtr", "%", "fr√©quence"]):
                corrected_df[col] = 0.0
            else:
                corrected_df[col] = 0
        
        warnings.append(f"Colonnes manquantes ajout√©es avec valeurs par d√©faut: {list(missing_columns)}")
    
    # Signaler les colonnes suppl√©mentaires (mais les garder)
    if extra_columns:
        warnings.append(f"Colonnes suppl√©mentaires d√©tect√©es: {list(extra_columns)}")
    
    # R√©organiser les colonnes dans l'ordre attendu
    final_columns = []
    for col in expected_columns:
        if col in corrected_df.columns:
            final_columns.append(col)
    
    # Ajouter les colonnes suppl√©mentaires √† la fin
    for col in extra_columns:
        final_columns.append(col)
    
    corrected_df = corrected_df[final_columns]
    
    # V√©rifier les types de donn√©es
    type_errors = []
    for col in corrected_df.columns:
        if col == "Date":
            if not pd.api.types.is_datetime64_any_dtype(corrected_df[col]):
                try:
                    corrected_df[col] = pd.to_datetime(corrected_df[col])
                except:
                    type_errors.append(f"Impossible de convertir '{col}' en datetime")
        elif any(keyword in col.lower() for keyword in ["tx", "vtr", "%", "fr√©quence", "temps"]):
            if not pd.api.types.is_numeric_dtype(corrected_df[col]):
                try:
                    corrected_df[col] = pd.to_numeric(corrected_df[col], errors='coerce').fillna(0.0)
                except:
                    type_errors.append(f"Impossible de convertir '{col}' en float")
        else:
            if not pd.api.types.is_numeric_dtype(corrected_df[col]):
                try:
                    corrected_df[col] = pd.to_numeric(corrected_df[col], errors='coerce').fillna(0).astype(int)
                except:
                    type_errors.append(f"Impossible de convertir '{col}' en int")
    
    if type_errors:
        errors.extend(type_errors)
    
    # Construire le message de r√©sultat
    messages = []
    if warnings:
        messages.extend([f"‚ö†Ô∏è {w}" for w in warnings])
    if errors:
        messages.extend([f"‚ùå {e}" for e in errors])
    
    is_valid = len(errors) == 0
    error_message = "\n".join(messages) if messages else "‚úÖ Toutes les colonnes sont valides"
    
    logger.info(f"Validation des colonnes: {len(corrected_df.columns)} colonnes, {len(corrected_df)} lignes")
    if messages:
        for msg in messages:
            logger.warning(msg)
    
    return is_valid, error_message, corrected_df

def validate_and_format_for_sheets(df):
    """
    Fonction combin√©e qui formate ET valide le DataFrame avant envoi vers Google Sheets
    AVEC CORRECTION FORC√âE DES NOMS
    """
    try:
        # 1. Formater d'abord
        formatted_df = format_dataframe(df)
        
        # 2. Ensuite valider (avec correction forc√©e int√©gr√©e)
        is_valid, validation_message, corrected_df = validate_dataframe_columns(formatted_df)
        
        # 3. V√©rifications suppl√©mentaires sp√©cifiques √† Google Sheets
        sheet_validations = []
        
        # V√©rifier qu'on a des donn√©es
        if corrected_df.empty:
            return False, "‚ùå DataFrame vide - aucune donn√©e √† envoyer", corrected_df
        
        # V√©rifier la colonne Date
        if "Date" not in corrected_df.columns:
            return False, "‚ùå Colonne 'Date' manquante - obligatoire pour Google Sheets", corrected_df
        
        # V√©rifier qu'on a au moins quelques m√©triques
        numeric_cols = corrected_df.select_dtypes(include=[int, float]).columns
        if len(numeric_cols) < 5:
            sheet_validations.append("‚ö†Ô∏è Peu de colonnes num√©riques d√©tect√©es")
        
        # V√©rifier les valeurs nulles dans les colonnes critiques
        critical_cols = ["Date", "Affichages de la page", "Visiteurs de la page"]
        null_checks = []
        for col in critical_cols:
            if col in corrected_df.columns:
                null_count = corrected_df[col].isnull().sum()
                if null_count > 0:
                    null_checks.append(f"'{col}': {null_count} valeurs nulles")
        
        if null_checks:
            sheet_validations.append(f"‚ö†Ô∏è Valeurs nulles d√©tect√©es: {', '.join(null_checks)}")
        
        # Construire le message final
        final_messages = [validation_message]
        if sheet_validations:
            final_messages.extend(sheet_validations)
        
        final_message = "\n".join(final_messages)
        
        # Le DataFrame est pr√™t si pas d'erreurs critiques
        success = is_valid and not corrected_df.empty and "Date" in corrected_df.columns
        
        logger.info(f"üéØ Validation finale - Colonnes: {list(corrected_df.columns)}")
        
        return success, final_message, corrected_df
        
    except Exception as e:
        error_msg = f"‚ùå Erreur lors de la validation: {str(e)}"
        logger.error(error_msg)
        return False, error_msg, df

def get_page_token(user_token, page_id):
    """
    R√©cup√®re le token sp√©cifique √† une page Facebook
    Version am√©lior√©e qui utilise d'abord les tokens sauvegard√©s
    """
    import json
    import os
    from datetime import datetime
    
    # D'abord essayer de r√©cup√©rer le token depuis la configuration sauvegard√©e
    try:
        # Essayer avec ConfigManager si disponible (Cloud Functions)
        try:
            from utils.config_manager import ConfigManager
            config_manager = ConfigManager()
            tokens_config = config_manager.load_config("page_tokens.json")
            
            if tokens_config and page_id in tokens_config.get("tokens", {}):
                saved_token = tokens_config["tokens"][page_id].get("access_token")
                if saved_token:
                    logger.info(f"Token trouv√© dans la configuration pour la page {page_id}")
                    return saved_token
        except ImportError:
            # En local, charger depuis le fichier
            if os.path.exists("configs/page_tokens.json"):
                with open("configs/page_tokens.json", 'r') as f:
                    tokens_config = json.load(f)
                    
                if page_id in tokens_config.get("tokens", {}):
                    saved_token = tokens_config["tokens"][page_id].get("access_token")
                    if saved_token:
                        logger.info(f"Token trouv√© dans le fichier local pour la page {page_id}")
                        return saved_token
    except Exception as e:
        logger.warning(f"Impossible de charger les tokens sauvegard√©s: {e}")
    
    # Si pas de token sauvegard√©, utiliser l'API Facebook
    logger.info(f"R√©cup√©ration du token via l'API pour la page {page_id}")
    url = f"https://graph.facebook.com/v21.0/me/accounts"
    params = {"access_token": user_token}
    
    try:
        response = requests.get(url, params=params)
        
        # V√©rifier le status code avant de lever une exception
        if response.status_code != 200:
            raise Exception(f"API returned status {response.status_code}: {response.text}")
            
        data = response.json()
        
        if "error" in data:
            raise Exception(f"Erreur API: {data['error'].get('message')}")
        
        for page in data.get("data", []):
            if page["id"] == page_id:
                page_token = page["access_token"]
                
                # Sauvegarder ce token pour les prochaines utilisations
                try:
                    # Essayer de sauvegarder avec ConfigManager
                    try:
                        from utils.config_manager import ConfigManager
                        config_manager = ConfigManager()
                        tokens_config = config_manager.load_config("page_tokens.json") or {"tokens": {}}
                        
                        tokens_config["tokens"][page_id] = {
                            "page_name": page.get("name", "Unknown"),
                            "access_token": page_token
                        }
                        tokens_config["last_updated"] = datetime.now().isoformat()
                        
                        config_manager.save_config("page_tokens.json", tokens_config)
                        logger.info(f"Token sauvegard√© pour la page {page_id}")
                    except ImportError:
                        # En local
                        os.makedirs("configs", exist_ok=True)
                        tokens_file = "configs/page_tokens.json"
                        
                        if os.path.exists(tokens_file):
                            with open(tokens_file, 'r') as f:
                                tokens_config = json.load(f)
                        else:
                            tokens_config = {"tokens": {}}
                        
                        tokens_config["tokens"][page_id] = {
                            "page_name": page.get("name", "Unknown"),
                            "access_token": page_token
                        }
                        tokens_config["last_updated"] = datetime.now().isoformat()
                        
                        with open(tokens_file, 'w') as f:
                            json.dump(tokens_config, f, indent=2)
                        logger.info(f"Token sauvegard√© localement pour la page {page_id}")
                except Exception as e:
                    logger.warning(f"Impossible de sauvegarder le token: {e}")
                
                return page_token
        
        raise Exception(f"Page {page_id} non trouv√©e ou non autoris√©e")
        
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration du token de page: {e}")
        raise

def get_earliest_available_date(access_token, page_id):
    """
    Trouve la date la plus ancienne o√π des donn√©es sont disponibles pour cette page
    """
    logger.info(f"üîç Recherche de la date la plus ancienne disponible pour la page {page_id}...")
    
    try:
        # Chercher en remontant par tranches d'un an
        current_year = datetime.now().year
        earliest_found = None
        
        for year_offset in range(10):  # Chercher jusqu'√† 10 ans en arri√®re
            test_start_date = datetime(current_year - year_offset, 1, 1)
            test_end_date = datetime(current_year - year_offset, 12, 31)
            
            if test_end_date > datetime.now():
                test_end_date = datetime.now()
            
            logger.info(f"   - Test de l'ann√©e {current_year - year_offset}...")
            
            # Tester avec une m√©trique simple
            test_url = f"https://graph.facebook.com/v21.0/{page_id}/insights"
            test_params = {
                "metric": "page_impressions",
                "period": "day",
                "access_token": access_token,
                "since": int(test_start_date.timestamp()),
                "until": int(test_end_date.timestamp()),
                "limit": 10  # R√©cup√©rer quelques points pour trouver la vraie premi√®re date
            }
            
            try:
                test_response = requests.get(test_url, params=test_params, timeout=10)
                if test_response.status_code == 200:
                    test_data = test_response.json()
                    if (test_data.get("data") and 
                        len(test_data["data"]) > 0 and 
                        test_data["data"][0].get("values") and
                        len(test_data["data"][0]["values"]) > 0):
                        
                        # Des donn√©es existent pour cette ann√©e, noter comme candidat
                        earliest_found = test_start_date
                        logger.info(f"‚úì Donn√©es trouv√©es pour l'ann√©e {current_year - year_offset}")
                        # Continuer √† chercher plus loin pour trouver le vrai d√©but
                        
            except Exception as test_error:
                logger.debug(f"Erreur lors du test de l'ann√©e {current_year - year_offset}: {test_error}")
            
            time.sleep(0.5)  # √âviter de surcharger l'API
        
        if earliest_found:
            logger.info(f"‚úÖ Date la plus ancienne trouv√©e: {earliest_found.strftime('%Y-%m-%d')}")
            return earliest_found
        else:
            # Si aucune donn√©e trouv√©e, utiliser une date par d√©faut raisonnable
            fallback_date = datetime.now() - timedelta(days=365)  # 1 an en arri√®re
            logger.info(f"üîÑ Aucune donn√©e historique trouv√©e, utilisation de la date de fallback: {fallback_date.strftime('%Y-%m-%d')}")
            return fallback_date
        
    except Exception as e:
        logger.warning(f"Erreur lors de la recherche de la date la plus ancienne: {e}")
        # Dernier recours: 1 an en arri√®re
        fallback_date = datetime.now() - timedelta(days=365)
        logger.info(f"Utilisation de la date de fallback: {fallback_date.strftime('%Y-%m-%d')}")
        return fallback_date

def get_page_creation_date(access_token, page_id):
    """
    Fonction conserv√©e pour compatibilit√© - redirige directement vers get_earliest_available_date
    NE TENTE PLUS d'acc√©der √† created_time via l'API Facebook
    """
    logger.info(f"üîÑ Utilisation de get_earliest_available_date pour la page {page_id}")
    return get_earliest_available_date(access_token, page_id)

def process_page_metrics(token, page_id, page_name):
    """
    Fonction principale pour traiter les m√©triques d'une page Facebook
    VERSION CORRIG√âE - GESTION APPROPRI√âE DE L'HISTORIQUE
    
    Args:
        token: Token utilisateur Facebook
        page_id: ID de la page Facebook
        page_name: Nom de la page Facebook
        
    Returns:
        spreadsheet_id: ID du Google Sheet cr√©√©/mis √† jour (ou None si erreur bloquante)
    """
    try:
        logger.info(f"üöÄ D√©but du traitement des m√©triques pour {page_name} ({page_id})")
        
        # R√©cup√©rer le token de la page
        page_access_token = get_page_token(token, page_id)
        
        # Initialiser le collecteur de base
        collector = FacebookBaseCollector(page_access_token)
        
        # Obtenir ou cr√©er le spreadsheet
        spreadsheet_id = collector.get_or_update_spreadsheet(page_name, page_id, "page_metrics")
        
        # D√©finir la date de fin (aujourd'hui)
        end_date = datetime.now()

        # R√©cup√©rer la date la plus ancienne disponible pour cette page
        earliest_available_date = get_earliest_available_date(page_access_token, page_id)
        
        # V√©rifier les dates existantes dans le spreadsheet
        existing_dates = collector.get_existing_dates(spreadsheet_id)
        logger.info(f"Dates existantes trouv√©es: {len(existing_dates)}")

        # Analyser la situation des donn√©es existantes
        update_mode = "complete_overwrite"  # Par d√©faut, collecte compl√®te
        start_date = earliest_available_date
        
        if len(existing_dates) == 0:
            # CAS 1: Aucune donn√©e - collecte compl√®te
            logger.info(f"üÜï Aucune donn√©e existante - collecte compl√®te depuis la date la plus ancienne disponible")
            update_mode = "complete_overwrite"
            
        else:
            # Normaliser et parser les dates existantes
            parsed_dates = []
            for d in existing_dates:
                try:
                    date_str = str(d).strip()
                    if date_str and date_str != '' and date_str.lower() != 'nan':
                        parsed_date = pd.to_datetime(date_str).date()
                        parsed_dates.append(parsed_date)
                except Exception as e:
                    logger.debug(f"Impossible de parser la date '{d}': {e}")
                    continue
            
            if not parsed_dates:
                # CAS 2: Donn√©es corrompues - collecte compl√®te
                logger.warning("Donn√©es existantes corrompues - collecte compl√®te depuis la date la plus ancienne disponible")
                update_mode = "complete_overwrite"
                # start_date est d√©j√† d√©fini √† earliest_available_date
                
            else:
                # Analyser les donn√©es existantes
                earliest_existing_date = min(parsed_dates)
                latest_existing_date = max(parsed_dates)
                
                logger.info(f"üìä Analyse des donn√©es existantes:")
                logger.info(f"   - Date la plus ancienne disponible (Facebook): {earliest_available_date.strftime('%Y-%m-%d')}")
                logger.info(f"   - Date la plus ancienne en base: {earliest_existing_date}")
                logger.info(f"   - Date la plus r√©cente en base: {latest_existing_date}")
                
                # CAS 3: V√©rifier si on a l'historique complet depuis la date disponible
                available_date_only = earliest_available_date.date()
                days_missing_from_start = (earliest_existing_date - available_date_only).days
                
                if days_missing_from_start > 30:  # Tol√©rance de 30 jours
                    # Il manque trop de donn√©es historiques - collecte compl√®te
                    logger.warning(f"‚ö†Ô∏è Historique incomplet: il manque {days_missing_from_start} jours depuis la date la plus ancienne disponible")
                    logger.info(f"üîÑ Remplacement complet des donn√©es pour avoir l'historique complet")
                    update_mode = "complete_overwrite"
                    
                elif earliest_existing_date < available_date_only:
                    # Les donn√©es existantes commencent avant la date disponible - c'est PR√âCIEUX !
                    logger.info(f"‚úÖ Donn√©es historiques pr√©cieuses d√©tect√©es: nous avons des donn√©es depuis {earliest_existing_date} alors que l'API ne va que jusqu'√† {available_date_only}")
                    logger.info(f"üéØ Conservation de l'historique existant et mise √† jour incr√©mentale")
                    
                    # Mise √† jour incr√©mentale depuis la derni√®re date existante
                    days_to_update = (datetime.now().date() - latest_existing_date).days
                    
                    if days_to_update <= 0:
                        logger.info(f"‚úÖ Les donn√©es sont d√©j√† √† jour pour {page_name}")
                        return spreadsheet_id
                    
                    logger.info(f"üîÑ Mise √† jour incr√©mentale - ajout des {days_to_update} derniers jours")
                    start_date = datetime.combine(latest_existing_date + timedelta(days=1), datetime.min.time())
                    update_mode = "incremental"
                    
                else:
                    # CAS 4: Historique semble correct - mise √† jour incr√©mentale
                    days_to_update = (datetime.now().date() - latest_existing_date).days
                    
                    if days_to_update <= 0:
                        logger.info(f"‚úÖ Les donn√©es sont d√©j√† √† jour pour {page_name}")
                        return spreadsheet_id
                    
                    logger.info(f"üîÑ Mise √† jour incr√©mentale - ajout des {days_to_update} derniers jours")
                    start_date = datetime.combine(latest_existing_date + timedelta(days=1), datetime.min.time())
                    update_mode = "incremental"
        
        # Log de la strat√©gie choisie
        if update_mode == "complete_overwrite":
            logger.info(f"üìã STRAT√âGIE: Collecte compl√®te du {start_date.strftime('%Y-%m-%d')} au {end_date.strftime('%Y-%m-%d')}")
        else:
            logger.info(f"üìã STRAT√âGIE: Ajout incr√©mental du {start_date.strftime('%Y-%m-%d')} au {end_date.strftime('%Y-%m-%d')}")

        # R√©cup√©rer les nouvelles donn√©es
        logger.info(f"üìä R√©cup√©ration des m√©triques du {start_date.strftime('%Y-%m-%d')} au {end_date.strftime('%Y-%m-%d')}")
        
        try:
            raw_data = fetch_page_metrics(page_access_token, METRICS, page_id, start_date, end_date)

            if raw_data.empty:
                logger.info(f"‚úÖ Aucune nouvelle donn√©e √† r√©cup√©rer pour {page_name}")
                return spreadsheet_id

            logger.info(f"üìà {len(raw_data)} nouvelles lignes de donn√©es r√©cup√©r√©es")

            # Formater et valider les donn√©es
            success, validation_message, validated_data = validate_and_format_for_sheets(raw_data)
            
            if not success:
                logger.error(f"‚ùå Validation √©chou√©e pour {page_name}: {validation_message}")
                return None
            
            logger.info(f"‚úÖ Validation r√©ussie pour {page_name}")
            if "‚ö†Ô∏è" in validation_message:
                logger.warning(f"Avertissements de validation: {validation_message}")

            # Envoyer les donn√©es vers Google Sheets selon la strat√©gie
            if update_mode == "complete_overwrite":
                # Remplacer compl√®tement les donn√©es existantes
                logger.info(f"üîÑ Remplacement complet des donn√©es...")
                collector.update_sheet_data(spreadsheet_id, validated_data)
                logger.info(f"‚úÖ Donn√©es remplac√©es compl√®tement pour {page_name}: {len(validated_data)} lignes")
            else:
                # Ajouter les nouvelles donn√©es √† la fin
                logger.info(f"üìä Ajout des nouvelles donn√©es...")
                collector.append_sheet_data(spreadsheet_id, validated_data)
                logger.info(f"‚úÖ {len(validated_data)} nouvelles lignes ajout√©es pour {page_name}")
            
            logger.info(f"üìã Colonnes finales: {list(validated_data.columns)}")

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la r√©cup√©ration/traitement des donn√©es pour {page_name}: {e}")
            import traceback
            logger.error(f"Stack trace: {traceback.format_exc()}")
            return None
        
        return spreadsheet_id
        
    except Exception as e:
        logger.error(f"‚ùå Erreur g√©n√©rale dans process_page_metrics pour {page_name}: {e}")
        import traceback
        logger.error(f"Stack trace: {traceback.format_exc()}")
        return None


# Pour les tests locaux uniquement
if __name__ == "__main__":
    import sys
    from dotenv import load_dotenv
    
    load_dotenv()
    
    # Configuration de logging pour les tests
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    if len(sys.argv) != 4:
        print("Usage: python fb_page_metrics.py <token> <page_id> <page_name>")
        sys.exit(1)
    
    test_token = sys.argv[1]
    test_page_id = sys.argv[2]
    test_page_name = sys.argv[3]
    
    try:
        spreadsheet_id = process_page_metrics(test_token, test_page_id, test_page_name)
        if spreadsheet_id:
            print(f"‚úì Test r√©ussi! Spreadsheet: https://docs.google.com/spreadsheets/d/{spreadsheet_id}")
        else:
            print("‚úó Test √©chou√© - aucun spreadsheet cr√©√©")
            sys.exit(1)
    except Exception as e:
        print(f"‚úó Erreur lors du test: {e}")
        sys.exit(1)