"""
Classe de base pour la collecte de donn√©es Facebook - Version optimis√©e pour les quotas
AVEC CORRECTION AUTOMATIQUE DES EN-T√äTES ET APPEND_SHEET_DATA
"""
import os
import json
import logging
from datetime import datetime
from googleapiclient.discovery import build
from google.oauth2 import service_account
import pandas as pd
import numpy as np
import time
import requests
from googleapiclient.errors import HttpError

logger = logging.getLogger(__name__)

class FacebookBaseCollector:
    """
    Classe de base pour la collecte de donn√©es Facebook avec gestion optimis√©e des quotas
    ET CORRECTION AUTOMATIQUE DES EN-T√äTES
    """
    # Variables de classe pour partager l'√©tat des quotas entre toutes les instances
    _last_api_call_time = 0
    _api_calls_count = 0
    _quota_reset_time = 0
    
    def __init__(self, page_token):
        self.page_token = page_token
        self.api_version = "v21.0"
        self.base_url = f"https://graph.facebook.com/{self.api_version}"
        self.token_manager = None
        
        # Param√®tres de gestion des quotas
        self.min_delay_between_calls = 1.5  # D√©lai minimum entre les appels API
        self.quota_calls_limit = 90  # Limite d'appels par 100 secondes (on garde une marge)
        self.quota_window = 100  # Fen√™tre de quota en secondes
        
        # Initialiser les services Google
        try:
            # Essayer d'abord les Application Default Credentials
            from google.auth import default
            credentials, project = default(
                scopes=['https://www.googleapis.com/auth/drive', 'https://www.googleapis.com/auth/spreadsheets']
            )
            logger.info("Utilisation des Application Default Credentials")
        except:
            # Fallback vers le fichier si disponible
            credentials_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS", "/workspace/credentials/service_account_credentials.json")
            if os.path.exists(credentials_path):
                credentials = service_account.Credentials.from_service_account_file(
                    credentials_path,
                    scopes=['https://www.googleapis.com/auth/drive', 'https://www.googleapis.com/auth/spreadsheets']
                )
                logger.info(f"Utilisation des credentials depuis {credentials_path}")
            else:
                # Dernier recours - essayer sans credentials explicites
                credentials = None
                logger.warning("Aucune credential trouv√©e, utilisation des credentials implicites")
        
        self.drive_service = build('drive', 'v3', credentials=credentials)
        self.sheets_service = build('sheets', 'v4', credentials=credentials)
        
        # Utiliser ConfigManager si disponible
        try:
            from utils.config_manager import ConfigManager
            self.config_manager = ConfigManager()
            self.use_cloud_storage = True
        except:
            self.config_manager = None
            self.use_cloud_storage = False
            os.makedirs("configs", exist_ok=True)
    
    def get_header_corrections_mapping(self):
        """
        Retourne les mappings de correction des en-t√™tes - VERSION COMPL√àTE
        """
        return {
            # Corrections communes √† tous les types
            "Nombre de fans": "Nbre de fans",
            "Nombre d'abonn√©s": "Nbre d'abonn√©s",
            "Fr√©quence des impressions": "Fr√©quence des affichages",
            "Fr√©quence impressions": "Fr√©quence des affichages",
            
            # üî• CORRECTIONS SP√âCIFIQUES pour posts_lifetime:
            "Clics totaux": "Nbre de clics",
            "post_activity_by_action_type_comment": "Nombre de commentaires",
            "post_activity_by_action_type_unique_comment": "Commentaires uniques",
            
            # Corrections suppl√©mentaires pour posts_metadata:
            "Nombre de commentaires": "Nbre de commentaires",
            "Nombre de J'aime": "Nbre de J'aime",
            "Nombre de partages": "Nbre de partages",
            
            # Autres corrections communes
            "Nombre de \"J'aime\"": "Nbre de \"J'aime\"",
            "Nombre de \"J'adore\"": "Nbre de \"J'adore\"",
            "Nombre de \"Wow\"": "Nbre de \"Wow\"",
            "Nombre de \"Haha\"": "Nbre de \"Haha\"",
            "Nombre de \"Triste\"": "Nbre de \"Triste\"",
            "Nombre de \"En col√®re\"": "Nbre de \"En col√®re\"",
        }
    
    def get_expected_headers_by_type(self, metric_type):
        """
        Retourne les en-t√™tes attendus selon le type de m√©trique - VERSION CORRIG√âE
        """
        if metric_type == "page_metrics":
            return [
                "Date",
                "Affichages de la page",
                "Visiteurs de la page", 
                "Affichages non viraux",
                "Affichages viraux",
                "Affichages des publications",
                "Visiteurs de la publication",
                "Affichages publicitaires",
                "Affichages organiques",
                "Visiteurs uniques organiques",
                "Vues totales de la page",
                "Nbre de fans",
                "Nouveaux fans",
                "Fans perdus",
                "Total nouveaux fans (payants + organiques)",
                "Nouveaux fans via pub",
                "Nouveaux fans organiques",
                "Nbre d'abonn√©s",
                "Nouveaux abonn√©s",
                "D√©sabonnements",
                "Abonn√©s uniques du jour",
                "Vues de vid√©os",
                "Vues uniques de vid√©os",
                "Vues vid√©os via pub",
                "Vues vid√©os organiques",
                "Relectures vid√©os",
                "Temps de visionnage (sec)",
                "Vues compl√®tes (30s)",
                "Vues compl√®tes uniques (30s)",
                "Vues compl√®tes via pub (30s)",
                "Vues compl√®tes organiques (30s)",
                "Vues compl√®tes auto (30s)",
                "Relectures compl√®tes (30s)",
                "Interactions sur publications",
                "Actions totales",
                "Nbre de \"J'aime\"",
                "Nbre de \"J'adore\"",
                "Nbre de \"Wow\"",
                "Nbre de \"Haha\"",
                "Nbre de \"Triste\"",
                "Nbre de \"En col√®re\"",
                "Tx d'engagement (%)",
                "Fr√©quence des affichages",
                "Actions calcul√©es",
                "VTR %"
            ]
        
        elif metric_type == "posts_lifetime":
            # üéØ BAS√â SUR VOS EN-T√äTES R√âELS:
            return [
                "Date de publication",
                "ID publication",
                "URL m√©dia",
                "Lien m√©dia",
                "Message",
                "Affichages publication",
                "Affichages organiques",
                "Affichages sponsoris√©s",
                "Affichages viraux",
                "Affichages par fans",
                "Affichages non viraux",
                "Visiteurs de la publication",
                "Visiteurs organiques",
                "Visiteurs via pub",
                "Visiteurs viraux",
                "Visiteurs non viraux",  # üîÑ Repositionn√© ici selon vos en-t√™tes
                "Nbre de \"J'aime\"",
                "Nbre de \"J'adore\"",
                "Nbre de \"Wow\"",
                "Nbre de \"Haha\"",
                "Nbre de \"Triste\"",
                "Nbre de \"En col√®re\"",
                "R√©actions J'aime",
                "R√©actions J'adore",
                "Nbre de clics",  # üî• CORRIG√â: √©tait "Clics totaux"
                "Autres clics",
                "Clics sur liens",
                "Clics sur photos",
                "Vues vid√©o",
                "Vues vid√©o organiques",
                "Vues vid√©o sponsoris√©es",
                "Visiteurs vid√©o uniques",
                "Visiteurs vid√©o organiques",
                "Visiteurs vid√©o sponsoris√©s",
                "Vues avec son",
                "Vues compl√®tes (30s)",
                "Temps moyen visionn√©",
                "Vues sur la page",
                "Vues via partages",
                "Port√©e fans",
                "Dur√©e totale visionnage",
                "Partages",
                "J'aime sur activit√©",
                "Partages uniques",
                "J'aime uniques",
                "Nouveaux abonn√©s vid√©o",
                "Interactions vid√©o",
                "Interactions totales",
                "Nbre de commentaires",  # üî• CORRIG√â: √©tait "post_activity_by_action_type_comment"
                "Commentaires uniques",    # ‚ûï AJOUT√â pour "post_activity_by_action_type_unique_comment"
                "Tx de clic (%)",
                "Tx d'engagement (%)",
                "R√©actions positives",
                "R√©actions n√©gatives",
                "Total r√©actions"
            ]
        
        elif metric_type == "posts_metadata":
            return [
                "ID publication",
                "Date de publication",
                "Type de publication",
                "Message",
                "Lien permanent",
                "Image",
                "Auteur",
                "ID auteur",
                "Nbre de commentaires",  # üî• CORRIG√â: coh√©rent avec les autres
                "Nbre de J'aime",        # üî• CORRIG√â: coh√©rent avec les autres
                "Nbre de partages"       # üî• CORRIG√â: coh√©rent avec les autres
            ]
        
        else:
            # Type par d√©faut ou autres types
            return []
    
    def correct_sheet_headers(self, spreadsheet_id, expected_headers, metric_type):
        """Corrige automatiquement les en-t√™tes du Google Sheet - VERSION CORRIG√âE"""
        try:
            # Lire les en-t√™tes actuels
            self._wait_for_quota()
            result = self.sheets_service.spreadsheets().values().get(
                spreadsheetId=spreadsheet_id,
                range="Metrics Data!1:1"  # Premi√®re ligne uniquement
            ).execute()
            
            current_headers = result.get('values', [[]])[0] if result.get('values') else []
            
            if not current_headers:
                logger.info("Aucun en-t√™te existant, pas de correction n√©cessaire")
                return False
            
            logger.info(f"üìã En-t√™tes actuels: {current_headers}")
            
            # Obtenir le mapping de correction
            correction_mapping = self.get_header_corrections_mapping()
            
            # Corriger les en-t√™tes
            corrected_headers = []
            headers_changed = False
            
            for header in current_headers:
                if header in correction_mapping:
                    corrected_header = correction_mapping[header]
                    corrected_headers.append(corrected_header)
                    headers_changed = True
                    logger.info(f"üîß Correction: '{header}' ‚Üí '{corrected_header}'")
                else:
                    corrected_headers.append(header)
            
            # V√©rifier s'il manque des en-t√™tes attendus
            for expected_header in expected_headers:
                if expected_header not in corrected_headers:
                    # Ajouter seulement si on n'a pas d√©j√† trop de colonnes
                    if len(corrected_headers) < 50:  # Limite raisonnable
                        corrected_headers.append(expected_header)
                        headers_changed = True
                        logger.info(f"‚ûï Ajout en-t√™te manquant: '{expected_header}'")
            
            # Appliquer les corrections si n√©cessaire
            if headers_changed:
                logger.info(f"üîÑ Mise √† jour des en-t√™tes...")
                
                # Mettre √† jour les en-t√™tes
                update_request = self.sheets_service.spreadsheets().values().update(
                    spreadsheetId=spreadsheet_id,
                    range="Metrics Data!1:1",
                    valueInputOption="USER_ENTERED",
                    body={'values': [corrected_headers]}
                )
                self._execute_with_retry(update_request, operation_name="correction des en-t√™tes")
                
                logger.info(f"‚úÖ En-t√™tes corrig√©s avec succ√®s!")
                logger.info(f"üéØ Nouveaux en-t√™tes: {corrected_headers}")
                
                return True
            else:
                logger.info("‚úÖ En-t√™tes d√©j√† corrects, aucune modification n√©cessaire")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la correction des en-t√™tes: {e}")
            return False

    def _wait_for_quota(self):
        """Attend si n√©cessaire pour respecter les quotas"""
        current_time = time.time()
        
        # V√©rifier si on doit r√©initialiser le compteur
        if current_time - FacebookBaseCollector._quota_reset_time > self.quota_window:
            FacebookBaseCollector._api_calls_count = 0
            FacebookBaseCollector._quota_reset_time = current_time
        
        # Si on approche de la limite, attendre
        if FacebookBaseCollector._api_calls_count >= self.quota_calls_limit:
            wait_time = self.quota_window - (current_time - FacebookBaseCollector._quota_reset_time)
            if wait_time > 0:
                logger.warning(f"Approche de la limite de quota ({FacebookBaseCollector._api_calls_count} appels), attente de {wait_time:.1f} secondes...")
                time.sleep(wait_time + 1)
                # R√©initialiser apr√®s l'attente
                FacebookBaseCollector._api_calls_count = 0
                FacebookBaseCollector._quota_reset_time = time.time()
        
        # Respecter le d√©lai minimum entre les appels
        time_since_last_call = current_time - FacebookBaseCollector._last_api_call_time
        if time_since_last_call < self.min_delay_between_calls:
            time.sleep(self.min_delay_between_calls - time_since_last_call)
        
        # Mettre √† jour les compteurs
        FacebookBaseCollector._last_api_call_time = time.time()
        FacebookBaseCollector._api_calls_count += 1
    
    def get_or_update_spreadsheet(self, page_name, page_id, metric_type):
        """Obtient ou cr√©e un spreadsheet avec gestion optimis√©e des quotas"""
        mapping_file = f"{metric_type}_mapping.json"
        
        if self.use_cloud_storage:
            mapping_config = self.config_manager.load_config(mapping_file) or {}
        else:
            try:
                with open(f"configs/{mapping_file}", 'r') as f:
                    mapping_config = json.load(f)
            except:
                mapping_config = {}

        # Si le spreadsheet existe d√©j√†
        if page_id in mapping_config:
            spreadsheet_id = mapping_config[page_id]["spreadsheet_id"]
            logger.info(f"Spreadsheet existant trouv√© pour {page_name} - {metric_type}")
            
            # NOUVEAU: V√©rifier et corriger les en-t√™tes si n√©cessaire
            expected_headers = self.get_expected_headers_by_type(metric_type)
            if expected_headers:
                logger.info(f"üîç V√©rification des en-t√™tes pour {metric_type}...")
                self.correct_sheet_headers(spreadsheet_id, expected_headers, metric_type)
            
            return spreadsheet_id

        # Si pas existant, on cr√©e le spreadsheet
        spreadsheet_id = self._create_spreadsheet(page_name, metric_type)

        # Sauvegarder le mapping
        mapping_config[page_id] = {
            "page_name": page_name,
            "spreadsheet_id": spreadsheet_id,
            "spreadsheet_url": f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}",
            "created_at": datetime.now().isoformat(),
            "last_updated": datetime.now().isoformat()
        }

        if self.use_cloud_storage:
            self.config_manager.save_config(mapping_file, mapping_config)
        else:
            with open(f"configs/{mapping_file}", 'w') as f:
                json.dump(mapping_config, f, indent=2)

        return spreadsheet_id

    def _create_spreadsheet(self, page_name, metric_type):
        """Cr√©e un nouveau spreadsheet avec gestion optimis√©e des quotas"""
        title = self._get_spreadsheet_title(page_name, metric_type)
        
        # Cr√©er le spreadsheet
        create_request = self.sheets_service.spreadsheets().create(
            body={
                'properties': {'title': title},
                'sheets': [{
                    'properties': {
                        'title': 'Metrics Data',
                        'gridProperties': {'frozenRowCount': 1}
                    }
                }]
            }
        )
        spreadsheet = self._execute_with_retry(create_request, operation_name="cr√©ation spreadsheet")
        spreadsheet_id = spreadsheet['spreadsheetId']
        
        # Donner acc√®s public en lecture
        permission_request = self.drive_service.permissions().create(
            fileId=spreadsheet_id,
            body={'type': 'anyone', 'role': 'reader'},
            fields='id'
        )
        self._execute_with_retry(permission_request, operation_name="permission publique")
        
        # Donner acc√®s √† l'email admin si sp√©cifi√© (optionnel, ne pas faire √©chouer si √ßa ne marche pas)
        admin_email = os.getenv("GOOGLE_ADMIN_EMAIL", "byteberry.analytics@gmail.com")
        if admin_email:
            try:
                admin_permission_request = self.drive_service.permissions().create(
                    fileId=spreadsheet_id,
                    body={'type': 'user', 'role': 'writer', 'emailAddress': admin_email},
                    fields='id'
                )
                self._execute_with_retry(admin_permission_request, max_retries=3, operation_name="permission admin")
            except:
                logger.warning(f"Impossible d'ajouter {admin_email} comme √©diteur - continuons sans")
                
        # Donner acc√®s au compte de service en writer
        service_account_email = os.getenv("SERVICE_ACCOUNT_EMAIL", f"facebook-automation@authentic-ether-457013-t5.iam.gserviceaccount.com")
        try:
            service_account_permission_request = self.drive_service.permissions().create(
                fileId=spreadsheet_id,
                body={'type': 'user', 'role': 'writer', 'emailAddress': service_account_email},
                fields='id'
            )
            self._execute_with_retry(service_account_permission_request, max_retries=3, operation_name="permission service account")
            logger.info(f"Permission ajout√©e pour le compte de service: {service_account_email}")
        except Exception as e:
            logger.warning(f"Impossible d'ajouter le compte de service comme √©diteur: {e}")

        logger.info(f"Nouveau spreadsheet cr√©√©: {spreadsheet_id}")
        return spreadsheet_id
    
    def append_sheet_data(self, spreadsheet_id, df):
        """
        üÜï NOUVELLE M√âTHODE - Ajoute des donn√©es √† la fin du sheet SANS √©craser l'existant
        """
        if df.empty:
            logger.warning("Aucune donn√©e √† ajouter")
            return
        
        try:
            # 1. D'abord v√©rifier qu'on a des en-t√™tes dans le sheet
            self._wait_for_quota()
            result = self.sheets_service.spreadsheets().values().get(
                spreadsheetId=spreadsheet_id,
                range="Metrics Data!1:1"  # Premi√®re ligne seulement
            ).execute()
            
            existing_headers = result.get('values', [[]])[0] if result.get('values') else []
            
            # 2. Pr√©parer les donn√©es SANS les en-t√™tes (car on ajoute seulement les donn√©es)
            df_clean = df.fillna("")
            data_rows = []
            
            for _, row in df_clean.iterrows():
                row_values = []
                for col, val in zip(df_clean.columns, row):
                    if isinstance(val, datetime):
                        row_values.append(val.strftime('%Y-%m-%d %H:%M:%S'))
                    elif isinstance(val, pd.Timestamp):
                        row_values.append(val.strftime('%Y-%m-%d %H:%M:%S'))
                    elif isinstance(val, str) and val.startswith('=IMAGE('):
                        url = val.replace('=IMAGE("', '').replace('")', '')
                        row_values.append(url)
                    elif isinstance(val, (int, np.integer)):
                        row_values.append(int(val))
                    elif isinstance(val, (float, np.floating)):
                        row_values.append(float(val))
                    elif isinstance(val, bool):
                        row_values.append('TRUE' if val else 'FALSE')
                    elif val == "" or pd.isna(val):
                        row_values.append("")
                    else:
                        row_values.append(str(val))
                data_rows.append(row_values)
            
            if not data_rows:
                logger.warning("Aucune ligne de donn√©es √† ajouter")
                return
            
            # 3. Si pas d'en-t√™tes existants, on ajoute d'abord les en-t√™tes
            if not existing_headers:
                logger.info("Aucun en-t√™te existant, ajout des en-t√™tes d'abord")
                headers = df_clean.columns.tolist()
                
                # Ajouter les en-t√™tes
                header_request = self.sheets_service.spreadsheets().values().update(
                    spreadsheetId=spreadsheet_id,
                    range="Metrics Data!A1",
                    valueInputOption="USER_ENTERED",
                    body={'values': [headers]}
                )
                self._execute_with_retry(header_request, operation_name="ajout en-t√™tes")
                logger.info(f"‚úÖ En-t√™tes ajout√©s: {len(headers)} colonnes")
                start_row = 2  # Commencer √† la ligne 2
            else:
                # 4. Trouver la prochaine ligne vide
                self._wait_for_quota()
                all_data_result = self.sheets_service.spreadsheets().values().get(
                    spreadsheetId=spreadsheet_id,
                    range="Metrics Data!A:A"  # Toute la colonne A
                ).execute()
                
                existing_rows = all_data_result.get('values', [])
                start_row = len(existing_rows) + 1  # Prochaine ligne vide
                
                logger.info(f"üìç Donn√©es existantes: {len(existing_rows)} lignes, ajout √† partir de la ligne {start_row}")
            
            # 5. Ajouter les nouvelles donn√©es
            logger.info(f"üìä Ajout de {len(data_rows)} nouvelles lignes √† partir de la ligne {start_row}")
            
            # Pour les gros volumes, utiliser des chunks
            chunk_size = 1000
            total_rows = len(data_rows)
            
            for i in range(0, total_rows, chunk_size):
                chunk_end = min(i + chunk_size, total_rows)
                chunk = data_rows[i:chunk_end]
                current_start_row = start_row + i
                
                if total_rows > chunk_size:
                    logger.info(f"üì• Ajout chunk {i//chunk_size + 1}/{(total_rows-1)//chunk_size + 1} (lignes {current_start_row}-{current_start_row + len(chunk) - 1})")
                
                append_request = self.sheets_service.spreadsheets().values().update(
                    spreadsheetId=spreadsheet_id,
                    range=f"Metrics Data!A{current_start_row}",
                    valueInputOption="USER_ENTERED",
                    body={'values': chunk}
                )
                self._execute_with_retry(append_request, operation_name=f"append chunk {i//chunk_size + 1}")
                
                # Petite pause entre les chunks
                if chunk_end < total_rows:
                    time.sleep(1)
            
            logger.info(f"‚úÖ {len(data_rows)} nouvelles lignes ajout√©es avec succ√®s!")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'ajout des donn√©es: {e}")
            raise
    
    def update_sheet_data(self, spreadsheet_id, df):
        """Met √† jour les donn√©es avec gestion robuste des gros volumes (√âCRASE tout)"""
        if df.empty:
            logger.warning("Aucune donn√©e √† uploader")
            return
        
        # Pr√©parer les donn√©es
        df = df.fillna("")
        values = [df.columns.tolist()]
        
        for _, row in df.iterrows():
            row_values = []
            for col, val in zip(df.columns, row):
                if isinstance(val, datetime):
                    row_values.append(val.strftime('%Y-%m-%d %H:%M:%S'))
                elif isinstance(val, pd.Timestamp):
                    row_values.append(val.strftime('%Y-%m-%d %H:%M:%S'))
                elif isinstance(val, str) and val.startswith('=IMAGE('):
                    url = val.replace('=IMAGE("', '').replace('")', '')
                    row_values.append(url)
                elif isinstance(val, (int, np.integer)):
                    row_values.append(int(val))
                elif isinstance(val, (float, np.floating)):
                    row_values.append(float(val))
                elif isinstance(val, bool):
                    row_values.append('TRUE' if val else 'FALSE')
                elif val == "" or pd.isna(val):
                    row_values.append("")
                else:
                    row_values.append(str(val))
            values.append(row_values)
        
        logger.info(f"Mise √† jour des donn√©es: {len(values)-1} lignes, {len(values[0])} colonnes")
        
        # D√©terminer la meilleure m√©thode selon la taille des donn√©es
        total_cells = len(values) * len(values[0]) if values else 0
        use_batch = total_cells < 10000  # Utiliser batch seulement pour moins de 10k cellules
        
        if use_batch:
            try:
                # Pour les petits datasets, essayer le mode batch
                self._update_sheet_batch(spreadsheet_id, values)
                logger.info("Donn√©es mises √† jour avec succ√®s en mode batch")
                return
            except Exception as e:
                if "Broken pipe" in str(e) or "timed out" in str(e):
                    logger.warning(f"Mode batch √©chou√© (donn√©es trop volumineuses): {e}")
                else:
                    logger.warning(f"Mode batch √©chou√©: {e}")
        
        # Pour les gros datasets ou si batch √©choue, utiliser le mode normal
        logger.info("Utilisation du mode normal (plus robuste pour gros volumes)")
        self._update_sheet_normal_chunked(spreadsheet_id, values)
        logger.info(f"Donn√©es mises √† jour: {len(df)} lignes")

    def _update_sheet_batch(self, spreadsheet_id, values):
        """Mode batch pour petits datasets - VERSION CORRIG√âE"""
        # D'abord effacer
        clear_request = self.sheets_service.spreadsheets().values().clear(
            spreadsheetId=spreadsheet_id,
            range="Metrics Data!A:ZZ"
        )
        self._execute_with_retry(clear_request, operation_name="clear for batch")
        
        # Puis mettre √† jour
        update_request = self.sheets_service.spreadsheets().values().update(
            spreadsheetId=spreadsheet_id,
            range="Metrics Data!A1",
            valueInputOption="USER_ENTERED",
            body={'values': values}
        )
        self._execute_with_retry(update_request, operation_name="batch update")
        
    def _update_sheet_normal_chunked(self, spreadsheet_id, values):
        """Mode normal avec d√©coupage en chunks pour gros volumes - VERSION CORRIG√âE"""
        # D'abord effacer toute la feuille
        clear_request = self.sheets_service.spreadsheets().values().clear(
            spreadsheetId=spreadsheet_id,
            range="Metrics Data!A:ZZ"
        )
        self._execute_with_retry(clear_request, operation_name="clear sheet")
        
        # Pour les tr√®s gros datasets, uploader par chunks
        chunk_size = 1000  # Lignes par chunk
        total_rows = len(values)
        
        if total_rows > chunk_size:
            logger.info(f"Dataset volumineux ({total_rows} lignes), upload par chunks de {chunk_size}")
            
            # Toujours inclure les headers dans le premier chunk
            for i in range(0, total_rows, chunk_size):
                chunk_end = min(i + chunk_size, total_rows)
                
                if i == 0:
                    # Premier chunk avec headers
                    chunk = values[0:chunk_end]
                    start_row = 1
                else:
                    # Chunks suivants sans headers
                    chunk = values[i:chunk_end]
                    start_row = i + 1  # +1 car on compte les headers
                
                logger.info(f"Upload chunk {i//chunk_size + 1}/{(total_rows-1)//chunk_size + 1} (lignes {start_row}-{start_row + len(chunk) - 1})")
                
                update_request = self.sheets_service.spreadsheets().values().update(
                    spreadsheetId=spreadsheet_id,
                    range=f"Metrics Data!A{start_row}",
                    valueInputOption="USER_ENTERED",
                    body={'values': chunk}
                )
                self._execute_with_retry(update_request, operation_name=f"update chunk {i//chunk_size + 1}")
                
                # Petite pause entre les chunks pour √©viter les probl√®mes
                if chunk_end < total_rows:
                    time.sleep(2)
        else:
            # Dataset normal, upload en une fois
            update_request = self.sheets_service.spreadsheets().values().update(
                spreadsheetId=spreadsheet_id,
                range="Metrics Data!A1",
                valueInputOption="USER_ENTERED",
                body={'values': values}
            )
            self._execute_with_retry(update_request, operation_name="update values")
            
    def _update_sheet_normal(self, spreadsheet_id, values):
        """Mode normal standard - VERSION CORRIG√âE"""
        # Nettoyer la feuille
        clear_request = self.sheets_service.spreadsheets().values().clear(
            spreadsheetId=spreadsheet_id,
            range="Metrics Data!A1:ZZ"
        )
        self._execute_with_retry(clear_request, operation_name="clear sheet")
        
        # Uploader les nouvelles donn√©es
        update_request = self.sheets_service.spreadsheets().values().update(
            spreadsheetId=spreadsheet_id,
            range="Metrics Data!A1",
            valueInputOption="USER_ENTERED",
            body={'values': values}
        )
        self._execute_with_retry(update_request, operation_name="update values")
        
    def _execute_with_retry(self, request, max_retries=7, operation_name=""):
        """Ex√©cute une requ√™te avec gestion des quotas et retry am√©lior√©e"""
        base_delay = 2
        
        for attempt in range(max_retries):
            try:
                # Attendre pour respecter les quotas
                self._wait_for_quota()
                
                result = request.execute()
                
                # Si succ√®s, on peut r√©duire l√©g√®rement le d√©lai minimum
                if self.min_delay_between_calls > 1:
                    self.min_delay_between_calls = max(1, self.min_delay_between_calls - 0.1)
                
                return result
                
            except HttpError as e:
                if e.resp.status == 429:  # Quota exceeded
                    # Augmenter le d√©lai minimum pour les prochains appels
                    self.min_delay_between_calls = min(5, self.min_delay_between_calls + 0.5)
                    
                    # Calculer le temps d'attente avec backoff exponentiel
                    wait_time = min(120, base_delay * (2 ** attempt))
                    
                    logger.warning(f"Quota d√©pass√© {operation_name} (tentative {attempt + 1}/{max_retries}), attente de {wait_time} secondes...")
                    time.sleep(wait_time)
                    
                    # R√©initialiser les compteurs apr√®s une erreur de quota
                    FacebookBaseCollector._api_calls_count = 0
                    FacebookBaseCollector._quota_reset_time = time.time()
                    continue
                    
                elif e.resp.status == 500:  # Internal server error
                    # Erreur serveur Google, attendre plus longtemps
                    wait_time = min(60, 10 * (attempt + 1))
                    logger.warning(f"Erreur serveur Google 500 {operation_name} (tentative {attempt + 1}/{max_retries}), attente de {wait_time} secondes...")
                    time.sleep(wait_time)
                    continue
                    
                elif e.resp.status == 503:  # Service unavailable
                    # Service temporairement indisponible
                    wait_time = min(60, 5 * (2 ** attempt))
                    logger.warning(f"Service indisponible 503 {operation_name} (tentative {attempt + 1}/{max_retries}), attente de {wait_time} secondes...")
                    time.sleep(wait_time)
                    continue
                    
                else:
                    logger.error(f"Erreur HTTP {e.resp.status} {operation_name}: {e}")
                    raise
                    
            except Exception as e:
                logger.error(f"Erreur inattendue {operation_name}: {e}")
                raise
        
        raise Exception(f"√âchec apr√®s {max_retries} tentatives {operation_name}")
    
    def _update_spreadsheet_title(self, spreadsheet_id, page_name, metric_type):
        """Met √† jour le titre - d√©sactiv√© pour √©conomiser les quotas"""
        # Comment√© pour √©conomiser les quotas
        pass
    
    def _format_columns_for_looker(self, spreadsheet_id, df):
        """Formatage d√©sactiv√© pour √©conomiser les quotas"""
        # Comment√© pour √©conomiser les quotas
        pass
    
    def _get_spreadsheet_title(self, page_name, metric_type):
        """G√©n√®re le titre du spreadsheet"""
        titles = {
            "page_metrics": f"Facebook Metrics - {page_name}",
            "posts_lifetime": f"Facebook Lifetime Posts Metrics - {page_name}",
            "posts_metadata": f"Facebook Metadata Posts Metrics - {page_name}"
        }
        
        base_title = titles.get(metric_type, f"Facebook {metric_type} - {page_name}")
        date = datetime.now().strftime('%Y-%m-%d')
        return f"{base_title} - Relev√© du {date}"
    
    # Les m√©thodes Facebook restent identiques
    def handle_api_error(self, error_response, context=""):
        """G√®re les erreurs de l'API Facebook"""
        if isinstance(error_response, dict) and "error" in error_response:
            error = error_response["error"]
            error_code = error.get("code", 0)
            error_subcode = error.get("error_subcode", 0)
            
            if error_code == 190 and error_subcode == 463:
                logger.error(f"Token expir√© d√©tect√© {context}: {error.get('message', '')}")
                return True
            
            if error_code == 190:
                logger.error(f"Erreur de token {context}: {error.get('message', '')}")
                return True
        
        return False
    
    def make_api_request(self, url, params, max_retries=3):
        """Effectue une requ√™te API Facebook avec gestion des erreurs"""
        for attempt in range(max_retries):
            try:
                response = requests.get(url, params=params, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                if self.handle_api_error(data, f"(tentative {attempt + 1}/{max_retries})"):
                    if self.token_manager and attempt < max_retries - 1:
                        logger.info("Tentative de rafra√Æchissement du token...")
                        try:
                            from utils.token_manager import FacebookTokenManager
                            tm = FacebookTokenManager()
                            new_token = tm.get_valid_token()
                            
                            params["access_token"] = new_token
                            self.page_token = new_token
                            
                            time.sleep(2)
                            continue
                        except Exception as e:
                            logger.error(f"Impossible de rafra√Æchir le token: {e}")
                    
                    raise Exception(f"Token expir√©: {data['error'].get('message', '')}")
                
                return data
                
            except requests.exceptions.RequestException as e:
                logger.error(f"Erreur r√©seau (tentative {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    raise
            except Exception as e:
                logger.error(f"Erreur inattendue: {e}")
                raise
        
        raise Exception(f"√âchec apr√®s {max_retries} tentatives")
    
    def get_all_spreadsheets_report(self):
        """G√©n√®re un rapport consolid√© de tous les spreadsheets"""
        all_spreadsheets = []
        metric_types = ["page_metrics", "posts_lifetime", "posts_metadata"]
        
        for metric_type in metric_types:
            mapping_file = f"{metric_type}_mapping.json"
            
            if self.use_cloud_storage:
                mapping_config = self.config_manager.load_config(mapping_file) or {}
            else:
                try:
                    with open(f"configs/{mapping_file}", 'r') as f:
                        mapping_config = json.load(f)
                except:
                    mapping_config = {}
            
            for page_id, info in mapping_config.items():
                spreadsheet_info = {
                    "page_id": page_id,
                    "page_name": info["page_name"],
                    "metric_type": metric_type,
                    "spreadsheet_id": info["spreadsheet_id"],
                    "spreadsheet_url": info.get("spreadsheet_url", f"https://docs.google.com/spreadsheets/d/{info['spreadsheet_id']}"),
                    "created_at": info.get("created_at"),
                    "last_updated": info.get("last_updated")
                }
                all_spreadsheets.append(spreadsheet_info)
        
        return {
            "generated_at": datetime.now().isoformat(),
            "total_spreadsheets": len(all_spreadsheets),
            "spreadsheets": all_spreadsheets
        }
    
    def get_existing_dates(self, spreadsheet_id):
        """
        R√©cup√®re la liste des dates d√©j√† pr√©sentes dans le Google Sheet - VERSION CORRIG√âE
        """
        try:
            self._wait_for_quota()  # Respecter quotas
            result = self.sheets_service.spreadsheets().values().get(
                spreadsheetId=spreadsheet_id,
                range="Metrics Data!A:A"  # Colonne A = 'Date'
            ).execute()

            values = result.get('values', [])
            if len(values) <= 1:
                # Soit vide, soit uniquement l'en-t√™te
                return set()

            dates = set()
            for row in values[1:]:  # Skip header
                if row and row[0]:
                    try:
                        parsed_date = pd.to_datetime(row[0]).strftime("%Y-%m-%d")
                        dates.add(parsed_date)
                    except:
                        # Si la conversion √©choue, on ignore cette cellule
                        continue

            logger.info(f"Dates existantes r√©cup√©r√©es : {len(dates)} dates")
            return dates

        except Exception as e:
            logger.warning(f"Impossible de r√©cup√©rer les dates existantes : {e}")
            return set()